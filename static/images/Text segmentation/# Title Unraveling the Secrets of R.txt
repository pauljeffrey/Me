# Title: Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 1)

## Introduction

In the realm of Natural Language Processing (NLP), engineers/data scientist can sometimes be faced with raw, unprocessed text which may present a unique challenge. Unlike structured or clean data, raw text may lack word boundaries, sentence boundaries, and proper noun identification. It's a jumbled mess of letters that can leave even the most seasoned NLP engineer scratching their heads.

But fear not, for I embarked on a captivating journey to develop a machine learning system that transforms this unstructured chaos into comprehensible, well-demarcated words and sentences. Through the magic of word segmentation, sentence segmentation, and capitalization, this system achieved an astonishing accuracy of approximately 97%! For this task, we will be making use of the brown corpus from the NLTK library.

A summary of what we want to achieve below:

```python
# Raw text
'danmorgantoldhimselfhewouldforgetannturnerhewaswellridofherhecertainlydidn'twantawifewhowasfickleasannifhehadmarriedherhe'dhavebeenaskingfortroublebutallofthiswasrationalizationsometime...'

# Model's final output
'dan morgan told himself he would forget ann turner he was well rid of her he certainly did n't want a wife who was fickle as ann if he had married her he 'd have been asking for trouble but all of this was rationalization sometime...'
```

Are you ready? Let's go!

## Data Download and Preprocessing
First, we import all the necessary libraries (we will be using tensorflow for our models here):

```python
import nltk
import sklearn
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder
import os
import pickle
from tensorflow.keras.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
```

Also, we will be making use of a small chunk of the brown text corpus found in the nltk library. We will remove all upper cases, sentence demarcations, word demarcations to get our raw text.

```python
nltk.download('brown')
from nltk.corpus import brown
from matplotlib import pyplot

nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('punkt')
```

First, let's check out the number of words in the brown corpus:

```python
print('Length of all words in all genres: ', len(brown.words()))

```
```
Length of all words in all genres:  1161192
```

Here, we see that there are more than 1 million words in all genre of the brown corpus. To model word segmentation, the list of characters will grow exponentially. Therefore, we will consider just a few number of genre to train our word segmentation model.

```python
# extract corpus for word segmentation training/sentence segmentation.
genres = brown.categories() # get genre categories
def extract_corpus(doc= 'char',num_of_genre= 9):
  corpus = '' #extract words from all genre into this variable
  if doc == 'char':
    for i in range(num_of_genre):
      corpus += ' '.join(brown.words(categories=genres[i]))
      corpus = corpus.replace('.','') 
      corpus = corpus.replace(',','') 
    return corpus
  else:
    corpus += ' '.join(brown.words(categories=genres[i]))
    corpus = corpus.replace('.','')
    corpus = corpus.replace(',','')
    return corpus

# extract Text
corpus = extract_corpus(num_of_genre=8)
print(len(corpus))
```

```
3817507
```

We will use this chunk to create our raw data. 


## Word Segmentation
The first step in our quest is to tackle word segmentation. This involves breaking down the continuous stream of characters into individual words. It's like taking a jumbled puzzle and painstakingly piecing it together, one word at a time.

To accomplish this, we must create a model that can segment raw texts (continuous stream of characters) into individual words. Therefore, we need to create a training dataset to train this model. 

(I employed a Conditional Random Field (CRF) model. CRFs are powerful statistical models that excel at sequence labeling tasks like word segmentation. By feeding the model with a sequence of characters and their corresponding features (such as part-of-speech tags), it learns to predict the boundaries between words.)

To do this, we will create characters (in the order they occur in the raw data) and targets for each character in our raw data. The target labels will be as follows:
- S for single word
- E for end of word
- I for inside word
- B for beginning of word. 

We could also add an extra tag for 'end of sentence' if we consider a joint (word and sentence segmentation). we will store the training data in an array. Then, we shall model this problem as a classification problem where we map each character to its corresponding target as stated above.
Here is the code to handle all of this:

```python
word_cat_targ = {
    'B' : 0, #
    'I' : 1,
    'E' : 2,
    'S' : 3
}
# Convert target to categorical variables
def to_cat(targ, cat_target):
  target = []
  for i in targ:
    target.append(cat_target[i])
  return np.array(target)

def create_train_data(corpus):
  # if ' ' exists before and after, its a single word
  # if ' ' exists only after character then it marks end of word
  # if ' ' exists only before character then its the beginning of the word
  # if ' ' does not exist before or after the character, then character exists within a word.
  train_data = []
  target_data = []
  length = len(corpus)
  for index,char in enumerate(corpus):
    # ignore space characters
    if char == ' ':
      continue

    train_data.append(char) # append character

    if index == 0:# if beginning of corpus, tag character as 'B'
      print(char)
      target_data.append('B')
      continue


    if index +1 < length : # If character is not the last character in the corpus
      #if space precedes and supersedes character
      if corpus[index-1] == ' ' and corpus[index+1] == ' ':
        target_data.append('S')
      # if space exist only before char
      elif corpus[index-1]== ' ' :
        target_data.append('B')
      # if space exists after character
      elif corpus[index+1] == ' ':
        target_data.append('E')
      # if no space before and after character
      else :
        target_data.append('I')
    # if last character in the corpus
    else:
      target_data.append('E')

  return train_data, target_data
```

Let's generate the data:
```python
train_data, target_data = create_train_data(corpus.lower())
target_data = to_cat(target_data, word_cat_targ)
```
Next, we do a little analysis on the training data. We store it in a dataframe for easy analysis.

```python
train_data = pd.DataFrame({'Characters': train_data})
train_data.tail()
```

```
table 1
```

### Model Objective:

Let's talk about the model a little. The task here is to classify each character of a sequence (raw text) as end of word E, beginning of word B, inside word I, single word S.

The major problem here is that these characters have no cues or have no features associated with them. Therefore, important features that will aid our model classify the characters accurately have to be generated.


### Feature Extraction and Engineering :

We need to generate features that will increase the accuracy of the classification probability. We will consider the character itself and the properties of the sequence (n-gram) of characters before and after it.

Using this method, we get not only the feature representation of the index character, but also the contextual information (in what context the character was used ) since we consider the characters existing before and after it. In this project, we will consider the following characteristics for feature engineering:


1. Features of the index character:

- one hot encoding representation
- type of char : letter, number, symbol (punctuation)
- class of char : vowel ,consonant, punctuation (e.g '!'), sign (e.g '$')

2. Properties of each character associated/surrounding the index character (This will serve as the context (sequence pattern) for the index character):

- One hot encoding representation
- type of char
- class of char


3. properties of sequence pattern (i.e pre-character(s), index character,post-character(s)).
We consider the characters that occur before and after the index character. To this, we consider the following:

4. frequency of sequence pattern in corpus.

5. Transitional probability: This is the probability of transitioning into another non-space character given the index character. We shall look at this property in both forward and backward direction to capture the pre and post characters. This is an intuitive entity. If the likelihood of seeing another non-space character after a particular index character is high, it means that both characters (index and one after) are more likely to be part of a word and so there should be no space between them. If its low, it means its less likely that these characters exist together in a word and so should be separated by a space forming different words. It is calculated by :

\[ \text{Probability}(x, y) = \frac{\text{Frequency}(x \text{ and } y \text{ occurring together})}{\text{Frequency}(x)} \]

where:
- \( x \) is the index character
- \( y \) is the pre or post-character

we can also take this a step further by calculating the probability of a sequence of character transitioning into another character. For example, given the word 'dinner', we can consider the transitional probability of the sequence 'di' transitioning into the letter 'n'. The higher this probability, the more likely that these three letters are part of a word and should not be separated by a space.

\[ \text{Probability}(\text{sequence} \rightarrow y) = \frac{\text{Frequency}(\text{sequence} \text{ followed by } y)}{\text{Frequency}(\text{sequence})} \]

6. probability of the sequence pattern : For example, the probability of the sequence pattern 'the' given the corpus if we consider a tri-gram pattern (3 character sequence) is given by :


\[ \text{Probability}(\text{pattern}) = \frac{\text{Frequency}(\text{pattern})}{\text{Total number of characters in corpus}} \]

Another is:

\[ \text{Probability}(\text{n-gram}) = \frac{\text{Frequency}(\text{n-gram})}{\text{Total number of occurring n-grams}} \]


- we may also consider the prob. of the class pattern of the sequence (e.g the character pattern 'the' is a 'consonant consonant vowel' pattern). It will be given by:

\[ \text{Probability}(\text{class pattern}) = \frac{\text{Frequency}(\text{class sequence})}{\text{Total number of sequence types}} \]

#### Note:

'n-gram' here refers to the character(s) before and after the current character. It was used here for convenience.


Since, the following models will be heavily reliant on frequencies and probabilities, the models will do way better with a large corpus!

Secondly, text processing will definitely be slower here because it will take more time extracting these features from a large enough dataset. Therefore, we trade computation, time for better perfomance of the model in all tasks in this notebook.


Lets check out the unique characters in the dataset. We have 55 unique characters in the dataset.

```python
print(len(train_data['Characters'].unique()))
train_data['Characters'].unique()
```

Now, we write some functions that will convert the meaningful properties of each character into class label encoding. Also as said earlier , we will create context for each characters by creating columns for the set of characters (n-gram) that exist both before and after them. This can be likened to helping the model see the future and past characters before making a decision.

This gives an idea of the likelihood of this sequence of string being part of a word (using the frequency of occurence of the sequence pattern in the whole corpus.

We do this by shifting the columns ('Character column') by a number of steps both up and down. We also use a custom label for all NaN values that appear in the newly created columns. This custom label will represent the null values.

For this first model, we use a 'UNK' to represent null values. We also use an n_gram of 2. This means we will extract the 2 characters before and after every character.

Create function that generates n-gram characters preceding and superceding the index character. We will work with just 1 or 2 characters pre and post index character. The reason is because since we are using a one hot encoding representation, we do not want too much of a sparse matrix as data for our model. We first define vowels, consonants, punctuations and symbols.

```
array(['d', 'a', 'n', 'm', 'o', 'r', 'g', 't', 'l', 'h', 'i', 's', 'e',
       'f', 'w', 'u', 'c', 'y', "'", 'k', 'v', 'b', 'z', 'p', 'x', ';',
       'j', 'q', '`', '?', '-', ':', '!', '2', '8', '(', ')', '&', '3',
       '0', '1', '9', '5', '$', '6', '7', '4', '%', '[', ']', '/', '*',
       '+', '{', '}'], dtype=object)
```

```python
vowels = ['a','e','i','o','u']
#numbers = ['0','1','2','3','4','5','6','7','8','9']
punctuations = [";","'",'?',':','!',';','"','.',',']

symbols = ['$','[',']','/','%','$','&','(',')','-','_','`','#','@','^','*','+','=','}','{',
        '>','<','~',"\\","|"]
```
Other preprocessing helper functions:
```python
# Function shifts the columns by a number of step so we can align any character with the character(s) that occur before and after it.
def extract_context(dataframe,column='Characters',name='',n_gram=1,fill_value= 'UNK'):
  for i in range(1,n_gram+1):
    dataframe[f'pre-{i} {name}'] = dataframe[column].shift(i, fill_value= fill_value) # shift down
    dataframe[f'post-{i} {name}'] = dataframe[column].shift(-i, fill_value=fill_value)# shift up
  return dataframe


# Function typify characters into 'alpha', 'dig', 'sym' for letters, numbers, symbols respectively.
def get_type(char):
  if char.isalpha():
    return 'a'
  elif char.isdigit():
    return 'd'
  else:
    return 's'

# Function classifies character into 'vow','con','dig','punc','sign' for vowel, consonant, number, punctuation and sign respectively
def get_class(char):
  if char.isdigit():
    return 'd'
  elif char in vowels:
    return 'v'

  elif char in punctuations:
    return 'p'
  elif char in symbols:
    return 's'
  else:
    return 'c'# character here has to be a consonant.

def get_congruity(dataframe):
  # This function checks the character before and after the index character to see if they belong to the
  # same type,class.
  dataframe['is_congruos_type'] = dataframe['pre-1 type'] == dataframe['post-1 type']
  dataframe['is_congrous_class'] = dataframe['pre-1 class'] == dataframe['post-1 type']
  return dataframe
```
Now, let's process these characters.

```python
train = extract_context(train_data.copy(), n_gram=2)
train.head()
```
```Table 2```

```python
train.tail(10)
```
```Table 3```

Next, we calculate and extract the frequency, prob of the index character, transitional probabilities with neighbouring characters, frequency and probability of whole sequence in the whole sequence. We can consider 3 character sequences and 5 character sequences.

First, we concatenate the sequences appropriately into new columns before computing the frequency of each. For sequences, we can consider 3 types of sequences:

Letter sequences (e.g 't','h','e' as sequence 'the')

class sequences (e.g 't','h','e' as sequence 'consonant-consonant-vowel')

type sequences (e.g 't', 'h','e' as sequence 'letter-letter-letter')

The most important is the first!

Let's write code for this (I'll leave exhaustive comments)
```python
char_properties = ['Characters','class','type'] # Properties of each character
def get_order(index_column='Characters',append_name='', n_gram=1):
  # Function gets the correct order of the characters that appear before and after the index character.
  order = [f'pre-{i} {append_name}' for i in range(1,n_gram+1)] + [index_column] +[f'post-{i} {append_name}' for i in range(1,n_gram+1)]
  return order


# This function generates all possible combinations (sequences) between index character and
# character(s) before and after.
# Note: This function is symmetric in action and was built to only consider exactly 1 or 2 characters
# before and after the index character.
# I did this to reduce the computational complexity. I also only used exactly 2 characters before and after
# the index character for this training.
def process_seq_columns(dataframe,column='Characters',append_name='',n_gram=1):
  order = get_order(column, append_name, n_gram)
  #print(order)
  if n_gram == 1: # If we are using just 1 character before and after, shift up by 1 step and down by 1 step
    dataframe[f'pre-1 seq {append_name}'] = dataframe[order[0]] + dataframe[order[1]]
    dataframe[f'post-1 seq {append_name}'] = dataframe[order[1]] + dataframe[order[2]]
    dataframe[f'whole-seq {append_name}'] = dataframe[order[0]] + dataframe[order[1]] + dataframe[order[2]]

  else: # Else shift 2 steps up and then down.
    dataframe[f'pre-1 seq {append_name}'] = dataframe[order[0]] + dataframe[order[2]]
    dataframe[f'pre-2 seq {append_name}'] = dataframe[order[1]] + dataframe[order[0]] + dataframe[order[2]]

    dataframe[f'post-1 seq {append_name}'] = dataframe[order[2]] + dataframe[order[3]]
    dataframe[f'post-2 seq {append_name}'] = dataframe[order[2]] + dataframe[order[3]] + dataframe[order[4]]
    dataframe[f'whole-seq {append_name}'] = dataframe[order[1]] + dataframe[order[0]] + dataframe[order[2]] + \
                            dataframe[order[3]] + dataframe[order[4]]

  return dataframe

# Function calculates the frequency and probability of occurrence of any given column
# It check the frequency of a character or sequence in the dataset and divides it by the number of samples in dataset or number of unique sequences in corpus.
# e.g how often the sequence 'to' occurs in the dataset given the number of unique 2 letter sequences in the dataset

def cal_freq_prob(dataframe,column, index=False,return_freq=False, seq=False):
  # group data by the unique values of a column and count their occurrence
  freq = dict(dataframe.groupby(column)[column].count())
  #print('length of freq keys: ', len(freq.keys()))
  if return_freq: # return frequency
    #print(column + '-freq')
    dataframe[column +'-freq'] = dataframe[column].apply(lambda x: int(freq[x]))


  num_of_samples = len(dataframe) # number of samples in dataframe

  if index:  # if its the 'Characters' column the find the percentage of occurence.
    dataframe[column + '-prob'] = dataframe[column].apply(lambda x : int(freq[x])/num_of_samples)
    return dataframe

  if seq: # if column represents a sequence then get the probability of sequence and occurence/number of unique (n_gram)sequence.
    # get the number of unique sequences after grouping by sequence column above
    num_of_seq = len(freq)
    #print(total)

    # Calculate both entities discussed above.
    dataframe[column + '-samprob'] = dataframe[column].apply(lambda x: int(freq[x])/ num_of_seq)
    dataframe[column + '-prob'] = dataframe[column].apply(lambda x : int(freq[x])/num_of_samples)


  return dataframe


# This function calculates the transitional probability using the formula as discussed above
# Function was initially for the words segmentation model but I later generalised the function so it can be used for the sentence  as well.
# given a sequence xyxf, we calculate the trans. prob. for y being the next letter given x, for x being the next letter given 'xy'
# and then for 'f' being the next letter given 'xyx'.
# All the columns ending with -freq suffix and pre/post prefix have already being automatically generated by other functions during
# processing.
# But they just create various sequence of diff. length given the n_gram: e.g for 'xyzf' we have : 'xy','xyz','xyzf' sequences.
def cal_transitional_prob(dataframe,index='Characters',n_gram=1):
  # if we are dealing with the 'Characters' column:
  if index == 'Characters':
    if n_gram == 1 : # if we consider just one character both before and after index character:
      dataframe['trans-pre'] = dataframe['pre-1 seq -freq'].astype(int)/dataframe[index+'-freq'].astype(int)
      dataframe['trans-post'] = dataframe['post-1 seq -freq'].astype(int)/ dataframe[index+'-freq'].astype(int)

    else: # if we consider more than one character
      for i in range(1, n_gram+1):
        if i == 1:
          dataframe['trans-pre-1'] = dataframe['pre-1 seq -freq'].astype(int)/dataframe[index+'-freq'].astype(int)
          dataframe['trans-post-1'] = dataframe['post-1 seq -freq'].astype(int)/ dataframe[index+'-freq'].astype(int)
        else:
          dataframe[f'trans-pre-{i}'] = dataframe[f'pre-{i} seq -freq'].astype(int)/dataframe[f'pre-{i-1} seq -freq'].astype(int)
          dataframe[f'trans-post-{i}'] = dataframe[f'post-{i} seq -freq'].astype(int)/ dataframe[f'pre-{i-1} seq -freq'].astype(int)

    return dataframe

  else: # If we are not dealing with 'Characters' column then do this below. Its basically the same code as that above.
    if n_gram == 1:
      dataframe[f'trans-pre {index}'] = dataframe[f'pre-1 seq {index}-freq'].astype(int)/dataframe[index+'-freq'].astype(int)
      dataframe[f'trans-post {index}'] = dataframe[f'post-1 seq {index}-freq'].astype(int)/ dataframe[index+'-freq'].astype(int)

    else:
      for i in range(1,n_gram+1):
        if i == 1: # if we are dealing with sequences that involve the closest set of characters to the index character.
        # for example: in 'xyxf', the closest characters to 'x' (3rd one) are 'y' and 'f'
          dataframe[f'trans-pre-{i} {index}'] = dataframe[f'pre-{i} seq {index}-freq'].astype(int)/dataframe[index+'-freq'].astype(int)
          dataframe[f'trans-post-{i} {index}'] = dataframe[f'post-{i} seq {index}-freq'].astype(int)/ dataframe[index+'-freq'].astype(int)
        else: # Else, consider the distant characters too
          dataframe[f'trans-pre-{i} {index}'] = dataframe[f'pre-{i} seq {index}-freq'].astype(int)/dataframe[f'pre-{i-1} seq {index}-freq'].astype(int)
          dataframe[f'trans-post-{i} {index}'] = dataframe[f'post-{i} seq {index}-freq'].astype(int)/ dataframe[f'pre-{i-1} seq {index}-freq'].astype(int)


  return dataframe
```
Now, we will make use of all the functions (put them all in one function) above to process the data and engineer our features in this format:

- Extract and process characters (remove '.',',' and uppercase etc)

- Extract properties of each features (e.g. alphabet, digit or symbol etc)

- calculate the frequency of each character in the corpus.

- Extract context : shift each character (a new column) by a number of steps so we can extract the character(s) that come before and after it.

- Calculate the frequency of occurence of the each character with the character(s) that come before and after it in the corpus.

- Check for congruity: If the characters before index character have the same type with the character after index character, return True otherwise, return false.

- Calculate the probability of the sequence being a valid subword from the corpus.

- Calculate the transitional probability of the index character to the next and previous characters. We do this for the characters.

- Encode columns with class labels (one-hot encoding).


After this processing, we will train with an MLP model for convenience. 


```python
# Function drops any set of columns we do not want to use for our task after we have computed and extracted the relevant features
def drop_column(dataframe, pattern): # pattern here represents a column name in dataset
    if type(pattern) == list: # if given a list of columns, do this: delete all.
      for i in pattern:
        for col in dataframe.columns:
          if col.endswith(i):
            dataframe = dataframe.drop(columns=[col])
    else: # else, delete the one column given
      for col in dataframe.columns:
        if col.endswith(pattern):
          dataframe = dataframe.drop(columns=[pattern])
    return dataframe

# Now, the main function that ties the whole task together: extract all features of index characters and its context.
def process_char_dataset(dataframe,char_fill= 'UNK', class_type_fill = 'UNK',n_gram=1):
  # block of code extractes properties of each character:
  dataframe['class'] = dataframe['Characters'].apply(get_class)
  dataframe['type'] = dataframe['Characters'].apply(get_type)
  dataframe = cal_freq_prob(dataframe, 'Characters',index=True,return_freq=True)

  # block of code extract context and its features
  dataframe = extract_context(dataframe, n_gram=n_gram,fill_value=char_fill)
  dataframe = extract_context(dataframe,'class','class',n_gram=n_gram,fill_value=class_type_fill)
  dataframe = extract_context(dataframe,'type','type',n_gram=n_gram, fill_value=class_type_fill)
  dataframe = get_congruity(dataframe)


  # block of code generates sequences by concatenating the index character to the 'before' and 'after' character(s)
  # block of code also calculates other properties : transitional prob, prob. of sequence etc

  dataframe = process_seq_columns(dataframe, n_gram= n_gram) # Create pre, post and whole sequences

  # Calculate frequency, prob of sequence and transitional probability for each character.
  for i in range(1, n_gram +1):
    #print('yes')
    dataframe = cal_freq_prob(dataframe, f'pre-{i} seq ',return_freq=True,seq=True) #for presequence
    dataframe = cal_freq_prob(dataframe, f'post-{i} seq ',return_freq=True,seq=True) # for post sequence

  dataframe = cal_freq_prob(dataframe,f'whole-seq ',return_freq=True,seq=True) # for whole sequence with index in middle

  # dataframe = cal_freq_prob(dataframe, 'pre-1 seq ',return_freq=True,seq=True) #for presequence
  # dataframe = cal_freq_prob(dataframe, 'post-1 seq ',return_freq=True,seq=True) # for post sequence
  # dataframe = cal_freq_prob(dataframe, 'whole-seq ',return_freq=True,seq=True) # for whole sequence with index in middle

  # calculate the transitional probabilities of all sequences/character into the character that immediately follows it.
  dataframe = cal_transitional_prob(dataframe, 'Characters' ,n_gram)

  # Now we drop all the columns we dont need after we have extracted and calculated the ones we need.

  drop = []
  for col in dataframe.columns :
    if (col.startswith('pre') and col.endswith('seq ')) or (col.startswith('post') and col.endswith('seq ')):
      drop.append(col)
  drop.append('whole-seq ')


  dataframe = drop_column(dataframe,drop)

  return dataframe
```

Next, we preprocess the data

```python
train = process_char_dataset(train,n_gram=2)
```

Let's look at the final data

```python
train.head()
```
```
Table 4
```

Next, we create our preprocessing transformers that will encode our categorical columns and scale our real valued columms. We will use the MinMax scaler for the real valued columns and the ordinal + one hot encoder for the categorical columns. Scaling is very important for deep learning frameworks as it gives a stable performance.


```python
# We store all categorical features in a list so we can apply the right encoding to them.
categorical_columns = ['Characters', 'class', 'type', 'pre-1 ', 'post-1 ','pre-2 ','post-2 ',
       'pre-1 class', 'post-1 class', 'pre-2 class','post-2 class','pre-1 type', 'post-1 type',
       'pre-2 type', 'post-2 type','is_congruos_type', 'is_congrous_class']

# Define categories that will be used by the Ordinal encoder.
# We add an extra character ',' to the unique values in the 'Characters' column so that
# 'Characters','pre-1','post-1' columns have the same kind of one-hot encoding representation.
# Therefore, we want both the 'Characters','type' and 'class' columns to have the same representation
# as their counterparts.
# It takes a dictionary of the colum to characters to append to represent the null_values
# So we define this dictionary before the function.
# I make this functionalities as functions so they can be reused downstream if needed.
append_chars = {'Characters': 'UNK',
                'class': 'UNK',
                'type': 'UNK'}

# Function as described above.
def get_categories(dataframe,append_chars):
  categories = []
  for cat in categorical_columns:
    if cat in append_chars.keys():
      values = list(dataframe[cat].unique())
      values.append(append_chars[cat])
      categories.append(values)
    else:
      categories.append(list(dataframe[cat].unique()))
  return categories


# To preprocess the data, we need to process the categorical features and continuous features in different ways
# so we write two classes that extract the categorical columns and drop the categorical columns so they can be processed
# differently by the different transformers.

class ColumnExtractor(BaseEstimator, TransformerMixin):
  def __init__(self,col_extract):
    super(ColumnExtractor, self).__init__()
    self.col_extract = col_extract

  def fit(self, X, y=None):
    return self

  def transform(self,X):
    return X[self.col_extract]

class ColumnDropper(BaseEstimator,TransformerMixin):
  def __init__(self,col_drop):
    super(ColumnDropper, self).__init__()
    self.col_drop = col_drop

  def fit(self, X, y=None):
    return self

  def transform(self,X):
    Xt = X.drop(columns=self.col_drop)
    return Xt

# function saves models to file.
def save_to_file(obj,file_path):
  with open(file_path,'wb') as f:
    pickle.dump(obj,f)
  return

# function loads saved models from file.
def load(file_path):
  with open(file_path, 'rb') as f:
    obj = pickle.load(f)
  return obj

# Function preprocesses and transforms data into sth acceptable by our deep learning model.
# we then store the fitted transformer to our drive so it can always be used anytime we want to segment raw text.
def preprocess_data(dataframe,cat_columns,cat_values=None,file_path=FILE_PATH,pipe_name=None,save=True):
  # define custom Transformers
  cat_transformer = ColumnExtractor(cat_columns) # extract categorical columns
  cont_transformer = ColumnDropper(cat_columns) # drop cat columns

  if cat_values:
    # define Scaler transformers appropriately
    scaler_pipeline = Pipeline([('transformer', cont_transformer),('scaler',MinMaxScaler())])
    encoder_pipeline = Pipeline([('transformer', cat_transformer),
                    ('label_encoder',OrdinalEncoder(categories=cat_values)),('one_hot',OneHotEncoder())])
  else:
    # define Scaler transformers appropriately
    scaler_pipeline = Pipeline([('transformer', cont_transformer),('scaler',MinMaxScaler())])
    encoder_pipeline = Pipeline([('transformer', cat_transformer),
                    ('label_encoder',OrdinalEncoder()),('one_hot',OneHotEncoder())])

  # create a pipeline that transforms the data simultaeneously
  feature_pipeline = FeatureUnion([('scaler_p',scaler_pipeline),('encoder_p',encoder_pipeline)])
  train_x = feature_pipeline.fit_transform(dataframe)

  # save transformer to file.
  if save and pipe_name:
    file_path = os.path.join(file_path,pipe_name)
    save_to_file(feature_pipeline,file_path)

  if save and (not pipe_name):
    file_path = os.path.join(file_path, 'demo')
    save_to_file(feature_pipeline, file_path)

  return train_x , feature_pipeline

```

```python
categories = get_categories(train,append_chars)
train , transformer = preprocess_data(train, categorical_columns,                       categories, pipe_name='char_pipeline_transformer')

train
```

```
<3088940x352 sparse matrix of type '<class 'numpy.float64'>'
	with 116829993 stored elements in Compressed Sparse Row format>
```

We have a sparse matrix because of the one hot encoding of a categorical variables with a lot of unique values.

```python
# We convert the target data into categorical data( one hot encoding) for training our deep learning model
target_data = tf.keras.utils.to_categorical(target_data)

# we split data into train and test set (0.8:0.2)
X_train, X_test , y_train, y_test = train_test_split(train, target_data,test_size=0.2,random_state=30, shuffle=False)

X_train, X_test
```

```
(<2471152x352 sparse matrix of type '<class 'numpy.float64'>'
 	with 93445997 stored elements in Compressed Sparse Row format>,
 <617788x352 sparse matrix of type '<class 'numpy.float64'>'
 	with 23383996 stored elements in Compressed Sparse Row format>)
```

## Train Model
```python

# define model, set batch size and number of epochs for training.
batch_size = 512
epochs = 10

input_dim = train.shape[1] # set the input dimensions for the neural network
output_dim = target_data.shape[1] # set the output_dim for the neural network

# This function creates a full model.
def create_model(input_dim,first_layer_units=500, hidden_layer_units=700,output_units=4):
  model = Sequential()
  model.add(Dense(first_layer_units,input_dim=input_dim ,activation='relu', kernel_initializer='he_uniform'))

  model.add(Dense(hidden_layer_units, activation='relu', kernel_initializer='he_uniform'))
  model.add(Dense(hidden_layer_units, activation='relu', kernel_initializer='he_uniform'))

  model.add(Dense(output_units, activation='softmax'))
  model.compile(loss= tf.keras.losses.CategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])
  #model.summary()

  return model

model = create_model(input_dim,1000,700,output_dim)
model.summary()
```

``
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 1000)              353000    
                                                                 
 dense_1 (Dense)             (None, 700)               700700    
                                                                 
 dense_2 (Dense)             (None, 700)               490700    
                                                                 
 dense_3 (Dense)             (None, 4)                 2804      
                                                                 
=================================================================
Total params: 1547204 (5.90 MB)
Trainable params: 1547204 (5.90 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```


```python
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# Schedule learning rate when you hit a plateau. The learning rate drops to a smaller value when performance hits a plateau.
lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_delta=1E-7)

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test),callbacks=[es,lr])

```
```
Epoch 1/10
4827/4827 [==============================] - 77s 14ms/step - loss: 0.2218 - accuracy: 0.9163 - val_loss: 0.1808 - val_accuracy: 0.9346 - lr: 0.0010
Epoch 2/10
4827/4827 [==============================] - 63s 12ms/step - loss: 0.1707 - accuracy: 0.9364 - val_loss: 0.1738 - val_accuracy: 0.9378 - lr: 0.0010
Epoch 3/10
4827/4827 [==============================] - 58s 11ms/step - loss: 0.1563 - accuracy: 0.9415 - val_loss: 0.1636 - val_accuracy: 0.9412 - lr: 0.0010
Epoch 4/10
4827/4827 [==============================] - 58s 11ms/step - loss: 0.1473 - accuracy: 0.9445 - val_loss: 0.1614 - val_accuracy: 0.9433 - lr: 0.0010
Epoch 5/10
4827/4827 [==============================] - 60s 12ms/step - loss: 0.1408 - accuracy: 0.9466 - val_loss: 0.1625 - val_accuracy: 0.9435 - lr: 0.0010
Epoch 6/10
4827/4827 [==============================] - 58s 11ms/step - loss: 0.1358 - accuracy: 0.9482 - val_loss: 0.1661 - val_accuracy: 0.9437 - lr: 0.0010
Epoch 7/10
4827/4827 [==============================] - 61s 11ms/step - loss: 0.1321 - accuracy: 0.9494 - val_loss: 0.1649 - val_accuracy: 0.9442 - lr: 0.0010
Epoch 8/10
4827/4827 [==============================] - 58s 11ms/step - loss: 0.1286 - accuracy: 0.9503 - val_loss: 0.1674 - val_accuracy: 0.9444 - lr: 0.0010
Epoch 9/10
4827/4827 [==============================] - 63s 12ms/step - loss: 0.1262 - accuracy: 0.9511 - val_loss: 0.1701 - val_accuracy: 0.9446 - lr: 0.0010
Epoch 9: early stopping
```

```python
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
```
```
word segmentor image here
```

Here, we see that the model performed very well and generalized pretty well to the data. However, its most likely that the performance of the model will differ based on the length of the full text it was given because of the features that were manually extracted for the model that took length (number of samples) into account.

Let's save the model to file memory.

```python
model_name = 'word_segmentor'
model_path = os.path.join(FILE_PATH,model_name)

model.save(model_path)
```

Whoops! That was a whole lot. We finally have a trained model for word segmentation with pretty good accuracy! Thats' all for word segmentation. To see and appreciate the output of this model, check out the <a href=""> Part 2 here</a>.

Thank you for your time and see you soon!!






# Title: Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2)

In the Part 1 of this article, we focused on training a Neural Network that can segment a raw sequence (stream) of text characters into words and we were able to achieve an accuracy of about 95% using some complex feature engineering. By the way, if you have not checked out the Part 1 of this lovely project's article, you can find it <a href=""> here </a>. I advice that you follow the first part of this project first before reading this. In this article, we are going to focus on predicting/forming full sentences given a corpus of words (without sentence demarcations) predicted by our "word segmentor" model from a raw sequence of text characters. We will call this text segmentation.

Example:
```My name is mary I am 10 years old I live in Nigeria```
is segmented into ```My name is mary. I am 10 years old. I live in Nigeria.```


## Introduction 

Once the words have been predicted by the "word segmentor" model, the next challenge is to segment these words into meaningful sentences. This is crucial for understanding the context and flow of the text.

For this task, we utilize a combination of rule-based and machine learning techniques. Rule-based methods involve defining a set of rules that identify sentence boundaries based on punctuation marks, conjunctions, and other linguistic cues. Machine learning methods, on the other hand, train a model on annotated data to predict sentence boundaries. We will be making use of the brown corpus (found in the NLTK library) as we did in the <a href="">last article</a>.

Let's go!

We will be preparing data mostly in the same format as in the previous article to extract features from data. Here, we are also going to be extracting features from the neighbouring words.

In addition to all the calculations we made in the previous article during feature extraction, we are going to make use of the 'Part of Speech' tags of each word here. This POS tags come with every word in the brown corpus. For the modelling of commas and full stop insertions, this POS tags play a huge role. For example, we know that many principles of use of the comma punctuation mark solely involves the identification of the part of speech of the words. We will also be using the length of words, tag congruity between neigbouring words etc. Everything is documented below. Please read through the code to understand the concepts and ideas.


```python
# This function extracts a list of list of tagged_sentences from the brown_corpus
# Given its argument, it gets the whole corpus or the number of genres provided.
def ext_tagged_corpus(num_of_genre=None):
  #print('yes')
  if num_of_genre: # if genre is given, then get only the sentences in that genre
    tagged_corpus = []
    for i in range(num_of_genre):
      tagged_corpus.extend(brown.tagged_sents(categories=brown.categories()[i]))
    return tagged_corpus

  else: # Else, get all sentences.
    return brown.tagged_sents()
```

For the sentences, we are going to tag a word with 'E' if it ends the sentence (comes before the full stop), and tag a word with 'P' (pause) if it comes before a comma and tag a word inside a sentence with an 'I'.

PS: Some functions here handle processing and generation of data for both sentence segmentation task and capitalization task which we will cover in part 3 of this series.

```python
sent_cat_targ = {'E' : 0, 'P': 1, 'I': 2}

#For true casing of words in sentences , we are going to tag a word as 'T' for titled if the #first letter of the word is in capital
# and tag it 'N' for not titled if the first letter isnt a capital letter.

case_cat_targ = {'T' : 0 , 'N': 1}

# This function creates the dataset, extracting all the words , part of speech tag associated with them and the target_tags in different lists.

def create_sent_data(tagged_sent,casing=False):
  words = []
  tags = []
  target_data = []

  if not casing: # if it is not the casing task (e.g John met the President) but segmentation task

    for each_sent in tagged_sent: # Go through each sentence in the list of sentences.
      for index, word in enumerate(each_sent): # Enumerate and go through words in each sentence
        if word[0] == '.' or  word[0] == ',': # if the current word is '.' or ',' then, continue to the next word
          continue

        if index < len(each_sent) -1: # if its not the last word ('.') in this sentence.

          if each_sent[index+1][0] == '.' : # if the current word is not the last and the word in front of the current word is a '.' , append the appropriate tag
            target_data.append('E')

          elif index != each_sent[-1] and each_sent[index+1] == ',': # Do the same for ','
            target_data.append('P')

          else: # Do the same for every other word
            target_data.append('I')

          words.append(word[0].lower()) # Append word in lower case
          tags.append(word[1])
  else:
    for each_sent in tagged_sent: # Go through each sentence in the list of sentences.

      for index, word in enumerate(each_sent): # Enumerate and go through words in each sentence

          if word[0].istitle() : # if the first letter of the word is cased
            target_data.append('T') # return 'T' for titled
          else: # Else return 'N' for not titled
            target_data.append('N')

          words.append(word[0].lower())
          tags.append(word[1])

  return words, tags, target_data # return all words, their tags and their target_data.

# Function that calculates the length of a word.
def word_length(word):
  return len(word)
```

Now, let's check out the length of our training corpus

```python
corpus = ext_tagged_corpus(8) # extract training corpus
len(corpus)
```

```
35104
```

Next, we extract words, tags and target data from this corpus below:

```python
words, tags , target_data = create_sent_data(corpus) # extract the words, various tags and target data
print(len(words) , len(target_data))
```

```
657589 657589
```

Let's take a look at the words and their corresponding tags.

```python
sent_df = pd.DataFrame({'words': words, 'tags': tags})
sent_df.head()
```

```
Table 1
```

Now, let's save the a list of the unique tags to a file permananently. We do this so that our models never return an error when they come across an entirely new data that has a tag it has never seen before. This way, we can set these tags to 'UNK' values.

```python
tag_list = sent_df['tags'].unique()
save_to_file(tag_list, os.path.join(FILE_PATH,'tags_list'))

print('Number of unique words in dataset: ', len(sent_df['words'].unique()))
print('Number of unique tags in dataset: ', len(sent_df['tags'].unique()))
````

```
Number of unique words in dataset:  38191
Number of unique tags in dataset:  437
```

We have about 38,191 unique words and it will be difficult to create a one_hot_encoding with this amount because the matrix will be very scarce. Instead, we could create a co-occurrence matrix (function defined below) to scale the data with StandardScaler() and then perform a PCA transform to reduce the dimensionality to a few hundred columns. This will help us represent the words uniquely. The co-occurrrence matrix will be a (38191 x 38191) matrix since we have 38,191 unique words in our extracted corpus. Then we can reduce this dimension using PCA to a few hundreds.

This will require setting a unique representation for any unknown (new) word the model will encounter during test. We can do this by replacing the least occuring word in the train set with an 'UNK' tag for example.

However, to reduce computational complexity, we are going to represent each word by its word length, frequency, probability in corpus, tagset,tagset of neigbouring words, transitional probabilities into neigbouring words and probabilities of the various sequences. We do this because we also know that the frequency and probability of the word in the whole corpus is unique to each word no matter where they occur.

I believe that this may reduce the performance of the model. This is because this model makes some assumptions. one of the very obvious one is that every unique word has a unique freq and probability of occurrence.


```python
from collections import defaultdict

# I do not advise this for this task though.
def co_occurrence(words, window_size): #
    d = defaultdict(int)
    vocab = set()
    for i,word in enumerate(words):
      vocab.add(word)  # add to vocab
      next_token = words[i+1 : i+1+window_size]
      for t in next_token:
        key = tuple( sorted([t, word]) )
        d[key] += 1

    # formulate the dictionary into dataframe
    vocab = sorted(vocab) # sort vocab
    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),
                      index=vocab,
                      columns=vocab)
    for key, value in d.items():
      df.at[key[0], key[1]] = value
      df.at[key[1], key[0]] = value
    return df

sent_df['word_length'] = sent_df['words'].apply(word_length) # calculate length of words.
sent_df.head()
```


Let's find the least occuring tag in the 'tags' column. We wiil use this to represent any new tag the transformer model we meet in the future when it encounters an entirely new text corpus.

```python
sent_df.groupby('tags').count()
```

```
Table 2
```

We see that the 'WRB+DOZ' occurs just once in the corpus. We shall use this as our fill_value.

```python
least_tag = 'WRB+DOZ'

# we convert the word_length to categories of exactly 1, 2 or greater than 2
def convert_length(word_length):
  if word_length == 1:
    return 0
  elif word_length == 2:
    return 1
  else:
    return 2

# Function gets the universal (non-specific) POS tags for each word. e.g 'NOUN' instead of 'NP' that stands for 'proper noun'.
def get_universal_tag(word):
  tag = nltk.pos_tag([word],tagset='universal')
  #print(tag)
  return tag[0][1]

# function checks if index word is a punctuation. We do this because we are sure that punctuations cant exist before a '.' or ','
def is_punc(word):
  if (word == '?' ) or (word == '!') or (word == ':') or (word == ';') or (word == '-'):
    return True
  else:
    return False

# function check if preceeding tag = tag of index character  = subsequent tag
# This very important for comma insertion . If the have the same tag, probably they are a set of listed items
# e.g I have mangoes ,apples, pineapples. This statement contains a list of 3 proper nouns.

def check_tag_congruity(row):
  if row['pre-1 tags'] == row['tags'] and row['tags'] == row['post-1 tags']:
    return 0
  elif row['pre-1 tags'] == row['tags']:
    return 1
  elif row['tags'] == row['post-1 tags']:
    return 2
  else:
    return 3

# This function process the whole data for the subsequent transformers in our pipeline.
# A lot of the functions used for the word segmentation model were also used here so they represent the same
# basic calculations and extractions.

def process_sent_data(df,n_gram=1,fill_value = 'WRB+DOZ'):
  df['word_length'] = df['words'].apply(word_length)
  df['word_length_class'] = df['word_length'].apply(convert_length)
  df['uni_tags']  = df['words'].apply(get_universal_tag)


  df = cal_freq_prob(df, 'words',index=True,return_freq=True) # frequency and prob of 'words'

  df = cal_freq_prob(df, 'tags', return_freq=True) # freq and prob of 'tags'

  # block of code extract context and its features (words around index word)
  # We are not bothered about the fill value for the words column here because the 'words' column will be deleted later.
  df = extract_context(df, 'words','words', n_gram=n_gram,fill_value=fill_value)
  df = extract_context(df,'tags','tags',n_gram=n_gram, fill_value=fill_value)

  # # block of code generates sequences by concatenating the index word to the 'before' and 'after' words
  # # block of code also calculates other properties : transitional prob, prob. of word sequence etc

  # Create pre, post and whole sequence
  df = process_seq_columns(df,'tags','tags',n_gram)
  df = process_seq_columns(df, 'words','words',n_gram)

  # Calculate frequency, prob of sequence and transitional probability for each character.

  # We check tag congruity between the pre-1 tag, index tag and post-1 tag
  df['tag_congruity']  = df.apply(check_tag_congruity,1)

  # Check that the current word is a punctuation mark
  df['is_punc'] = df['words'].apply(is_punc)

  # calculate frequency , probability of occurence of all extracted sequences.
  for col in ['tags', 'words']:
    for i in range(1, n_gram + 1):
      df = cal_freq_prob(df, f'pre-{i} seq {col}',return_freq=True,seq=True) #for presequence
      df = cal_freq_prob(df, f'post-{i} seq {col}',return_freq=True,seq=True) # for post sequence
      df = cal_freq_prob(df,f'whole-seq {col}',return_freq=True,seq=True) # for whole sequence with index in middle

  # calculate the trans. prob. of the index character to the next and for increasing order of sequence to the next character.
  # Do the same for the 'tags' column.
  df = cal_transitional_prob(df, 'words' ,n_gram)
  df = cal_transitional_prob(df, 'tags', n_gram)

  # Since we represent words here by their freq and prob, to give each word more uniqueness, we shift the columns up and down
  # By a number of steps so they get infomation about the characters that exist before and after them.
  df = extract_context(df,'words-freq','words-freq',n_gram=1, fill_value= 0)
  df = extract_context(df,'words-prob','words-prob',n_gram=1, fill_value= 0)

  # Now we drop all the columns we dont need after we have extracted and calculated the ones we need below:
  if n_gram == 1: # if we considered only one character pre and post
    drop = [col for col in df.columns if (col.endswith('-freq') and ('words' not in col))]
    drop.append('whole-seq words')
    drop.append('whole-seq tags')
    drop.append('pre-1 seq words')
    drop.append('pre-1 seq tags')
    drop.append('post-1 seq words')
    drop.append('post-1 seq tags')
    drop.append('post-1 words')
    drop.append('pre-1 words')

  else: # if we considered more than one character pre and post.
    drop = [col for col in df.columns if (col.endswith('-freq') and ('words' not in col))]  + \
        [ col for col in df.columns if (col.startswith('pre-2') and col.endswith('tags'))] + \
        [col for col in df.columns if (col.startswith('post-2') and col.endswith('tags'))] + \
        [col for col in df.columns if (col.startswith('pre-2') and col.endswith('words'))] + \
        [col for col in df.columns if (col.startswith('post-2') and col.endswith('words'))]# + \
        #[col for col in df.columns if (col.startswith('post-2') and col.endswith('tags'))]
    drop.append('whole-seq words')
    drop.append('whole-seq tags')
    drop.append('pre-1 seq words')
    drop.append('pre-1 seq tags')
    drop.append('post-1 seq words')
    drop.append('post-1 seq tags')
    drop.append('post-1 words')
    drop.append('pre-1 words')


  drop.append('words')

  df = drop_column(df, drop)

  return df

sent_cat = [ 'tags', 'word_length', 'uni_tags', 'pre-1 tags', 'post-1 tags', 'tag_congruity', 'is_punc']
```

Now, we compute and extract all our features so they can be preprocessed into the format that is acceptable to our machine learning model.

```python
train = process_sent_data(sent_df, 2) # process the data.
train.head()
```

```
Table 3
```

```python
print(len(train['tags'].unique()))
print(len(train['pre-1 tags'].unique()))
print(len(train['post-1 tags'].unique()))
```

```
437
437
437
```

Before preprocessing data, we need to include the 'UNK' tag we used to fill up the 'pre' and 'post' tags column we created to the 'tags' column. We do this so they all have equal representation dimensions.

```python
# Extract the unique values in categories and append 'UNK' to the tags column's unique values
sent_cat_values = [list(train[col].unique()) for col in sent_cat ]
sent_cat_values[0].append('UNK')

# We sort all the values in each category because the Ordinal Encoder will not accept numerical values that are not sorted.
for cat in sent_cat_values:
  cat.sort()
```

We then preprocess data using the same function we used for the word segmentation model.

```python

train , pipeline = preprocess_data(train, sent_cat, pipe_name='sentence_transformer')
train
```

```
<657589x1393 sparse matrix of type '<class 'numpy.float64'>'
	with 19760410 stored elements in Compressed Sparse Row format>
```

```python
# Convert target_data variables from string format to numerical format
target_data = to_cat(target_data, sent_cat_targ)

# change target to categorical format (One hot encoding)
target_data = tf.keras.utils.to_categorical(target_data)

# Split data into train and test set
X_train, X_test , y_train, y_test  = train_test_split(train, target_data, test_size= 0.2,
                                                      random_state=42,shuffle=False)
```

Now, Let's create our deep learning model for training. I also suggest the use of SVMs for training because they should work well as classifiers when data is represented in a high dimensional format. Xgboost is another option to consider.

```python
# Get input and output dim
input_dim = X_train.shape[1]
output_dim = 3 # because we have only 3 targets

model = create_model(input_dim, 1600, 1000, output_dim)
model.summary()
```

```
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_4 (Dense)             (None, 1600)              2230400   
                                                                 
 dense_5 (Dense)             (None, 1000)              1601000   
                                                                 
 dense_6 (Dense)             (None, 1000)              1001000   
                                                                 
 dense_7 (Dense)             (None, 3)                 3003      
                                                                 
=================================================================
Total params: 4835403 (18.45 MB)
Trainable params: 4835403 (18.45 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

Let's train the model now!
```python
# Schedule early stopping so that model can be stopped when there is no longer improvement
# after at least 5 steps.
epochs = 20
batch_size  = 512
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# Schedule learning rate when you hit a plateau
lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_delta=1E-7)

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test),callbacks=[es,lr])
```

```
Epoch 1/20
1028/1028 [==============================] - 27s 23ms/step - loss: 0.1124 - accuracy: 0.9577 - val_loss: 0.0985 - val_accuracy: 0.9631 - lr: 0.0010
Epoch 2/20
1028/1028 [==============================] - 22s 21ms/step - loss: 0.1006 - accuracy: 0.9607 - val_loss: 0.0956 - val_accuracy: 0.9639 - lr: 0.0010
Epoch 3/20
1028/1028 [==============================] - 16s 14ms/step - loss: 0.0965 - accuracy: 0.9619 - val_loss: 0.0952 - val_accuracy: 0.9644 - lr: 0.0010
Epoch 4/20
1028/1028 [==============================] - 16s 14ms/step - loss: 0.0928 - accuracy: 0.9631 - val_loss: 0.0959 - val_accuracy: 0.9637 - lr: 0.0010
Epoch 5/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0895 - accuracy: 0.9638 - val_loss: 0.1010 - val_accuracy: 0.9634 - lr: 0.0010
Epoch 6/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0861 - accuracy: 0.9651 - val_loss: 0.1011 - val_accuracy: 0.9638 - lr: 0.0010
Epoch 7/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0773 - accuracy: 0.9678 - val_loss: 0.1139 - val_accuracy: 0.9626 - lr: 1.0000e-04
Epoch 8/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0741 - accuracy: 0.9689 - val_loss: 0.1226 - val_accuracy: 0.9628 - lr: 1.0000e-04
Epoch 8: early stopping
```

Let's plot the traning history.
```python
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
```

```sentence segmentor image here```

Let's save our model.

```python
model_name = 'Comma_sentence_segmentor'
model_path = os.path.join(FILE_PATH,model_name)

model.save(model_path)

```

We see that our model generalizes well and has about 96.6% accuracy on the validation set!

That's all for this part. Check out the last part of this series <a href="">here</a>. It is going to focus on True casing (Capitalization). In the next article you will also get to see all of our trained models in action as they attempt to process a raw sequence stream of text characters to cased sentences which is very easy to read and understand.

See you <a href="">there</a>!.






# Title: Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 3)

## Introduction

The final piece of the puzzle is to capitalize the first letter of each sentence and other words that need to be cased (e.g nouns). This not only enhances readability but also provides valuable contextual information.
By the way, if you have not checked out the first 2 parts of this lovely project's article, you can find it <a href=""> here </a>. I advice that you follow the previous parts of this project first before reading this last one.

To achieve this, We are going to train a model using the following principles/assumptions of true casing of words:

- The first letter of the first word after a '.','!','?' (beginning of a sentence) is always capitalized.
- Names and other proper nouns should be capitalized.
- Words after a colon should not be capitalized.
- Capitalize the first word of a quote.
- Capitalize months , days but not seasons.

Therefore, we create functions to extract some of these information for our model. To save time, I will leave them here but please take your time to go through each of them to understand what they do.


```python
months = ['january', 'february','march','april','may','june','july','august','september','october','november','december']
days = ['monday','tuesday', 'wednesday','thursday','friday','saturday','sunday']

# These punctuations separate sentences in english language.
sent_separator = ['.','!','?','UNK']
# function checks if a word begins a sentence.
def begins_sentence(row):
  if row['pre-1 words'] in sent_separator:
    return True
  else:
    return False

# if the word is a month or day
def is_month_day(word):
  if ( word in months ) or (word in days):
    return True
  else:
    return False

# If the word is a proper noun
def is_proper_noun(word):
  if word == 'NNP' or word == 'NP':
    return True
  else:
    return False

# if the word comes after quotes
def supersede_quote(row):
  if row['pre-1 words'] == "'" or row['pre-1 words'] == '"':
    return True
  else:
    return False

# if the word comes after a colon
def after_colon(row):
  if row['pre-1 words'] == ':':
    return True

  else:
    return False

# function to check if word is a symbol or digit:
def check_digit_sym(word):
  if (word in punctuations) or (word in symbols) :
    return True
  elif word.isdigit():
    return True
  else:
    False

```
Now, we extract and process the train and test data

```python
corpus = ext_tagged_corpus(8) # extract corpus

words, tags, target_data = create_sent_data(corpus, casing=True)
del corpus
```

```python
sent_df = pd.DataFrame({'words': words, 'tags': tags}) # create dataframe
sent_df.head()
```
```
Table 1
```
Some functions from the word segmentation model were used here also.

```python


def process_case_data(df,n_gram=1,fill_value = 'WRB+DOZ'):

  # Extract all features as described earlier
  df['is_month_day'] = df['words'].apply(is_month_day)
  #df['is_digit_sym']  = df['words'].apply(check_digit_sym)
  df['is_proper_noun']  = df['tags'].apply(is_proper_noun)



  # block of code extract context and its features (words around index word)
  df = extract_context(df, 'words','words', n_gram=n_gram,fill_value=fill_value)
  df = extract_context(df, 'tags','tags', n_gram=n_gram,fill_value=fill_value)

  # Extract all features as described earlier
  df['after_colon'] = df.apply(after_colon,1)
  df['supersed_quote'] = df.apply(supersede_quote,1)
  df['begins_sentence'] = df.apply(begins_sentence,1)

  # Drop all irrelevant columns
  drop = ['post-1 words']
  drop.append('pre-1 words')
  drop.append('post-1 tags') # we remove the tags that come after word and leave the one that comes before.

  drop.append('words')

  df = drop_column(df, drop)

  return df

train = process_case_data(sent_df)
train.head()
```

```Table 2
```

```python
# Also save tags for this model
case_tag_list = train['tags']
save_to_file(case_tag_list, os.path.join(FILE_PATH,'case_tag_list'))
```

Since all columns are categorical , we pass it through an ordinal and one hot encoder first we append 'UNK' to the tags column for general representaton of tags.

```python

case_cat_value = [list(train[col].unique()) for col in train.columns]
case_cat_value[0].append('UNK')

for col in case_cat_value:
  try:
    col.sort()
  except:
    continue


# preprocess data by passing through encoder transformer

case_pipeline = Pipeline([('ordinal_encoder', OrdinalEncoder()),
                          ('one hot', OneHotEncoder())])
# Fit transformer to data.
train = case_pipeline.fit_transform(train)

# Save transformer to file
transformer_path = os.path.join(FILE_PATH,'case_pipeline')
save_to_file(case_pipeline,transformer_path)
```


Split data into train and test

```python
# Convert target_data variables from string format to numerical format
target_data = to_cat(target_data, case_cat_targ)

# change target to categorical format (One hot encoding)
target_data = tf.keras.utils.to_categorical(target_data)

# Split data into train and test set
X_train, X_test , y_train, y_test  = train_test_split(train, target_data, test_size= 0.15,
                                                      random_state=42,shuffle=False)

X_train
```

```
<619288x890 sparse matrix of type '<class 'numpy.float64'>'
	with 4335016 stored elements in Compressed Sparse Row format>
```

Again, we create our model with the previous function <a href="">Part 1</a>

```python
# Get input and output dim
input_dim = X_train.shape[1]
output_dim = 2 # because we have only 2 targets

model = create_model(input_dim, 1500, 1000, output_dim)
model.summary()
```

```
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_8 (Dense)             (None, 1500)              1336500   
                                                                 
 dense_9 (Dense)             (None, 1000)              1501000   
                                                                 
 dense_10 (Dense)            (None, 1000)              1001000   
                                                                 
 dense_11 (Dense)            (None, 2)                 2002      
                                                                 
=================================================================
Total params: 3840502 (14.65 MB)
Trainable params: 3840502 (14.65 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

Now, we train the model
```python
# Schedule early stopping so that model can be stopped when there is no longer improvement
# after at least 5 steps.
epochs = 20
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)

# Schedule learning rate when you hit a plateau
lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_delta=1E-7)

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test),callbacks=[es,lr])
```

```
Epoch 1/20
1210/1210 [==============================] - 22s 16ms/step - loss: 0.0424 - accuracy: 0.9875 - val_loss: 0.0368 - val_accuracy: 0.9895 - lr: 0.0010
Epoch 2/20
1210/1210 [==============================] - 13s 9ms/step - loss: 0.0363 - accuracy: 0.9894 - val_loss: 0.0371 - val_accuracy: 0.9905 - lr: 0.0010
Epoch 3/20
1210/1210 [==============================] - 17s 13ms/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 0.0343 - val_accuracy: 0.9906 - lr: 0.0010
Epoch 4/20
1210/1210 [==============================] - 14s 10ms/step - loss: 0.0353 - accuracy: 0.9896 - val_loss: 0.0349 - val_accuracy: 0.9906 - lr: 0.0010
Epoch 5/20
1210/1210 [==============================] - 13s 9ms/step - loss: 0.0350 - accuracy: 0.9896 - val_loss: 0.0354 - val_accuracy: 0.9905 - lr: 0.0010
Epoch 6/20
1210/1210 [==============================] - 13s 10ms/step - loss: 0.0347 - accuracy: 0.9897 - val_loss: 0.0361 - val_accuracy: 0.9902 - lr: 0.0010
Epoch 6: early stopping
```

Let's print the training history:
```python
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
```

``` casing image here ```

We save this model like this and calculate our metrics
```python
model_name = 'Casing_model'
model_path = os.path.join(FILE_PATH,model_name)

model.save(model_path)
```

Let's evaluate our model using accuracy, precision, recall, F1 score

```python
from sklearn.metrics import f1_score, precision_score , recall_score, accuracy_score
# Error Functions and evaluation

# Use this functions to perform f1_score, recall and precision for error analysis.
def calculate_metrics(cat_target , y_true, y_pred):
  # For each label classification , we create a case of 'has label' and 'has no label'.
  # For example, for the 'I' label, we convert all predictions into 1 for 'I' and 0 for not 'I'
  try:
    y_true = list(np.argmax(y_true, 1))
  except:
    print('True predictions must be in categorical format.')
    return

  y_pred = list(np.argmax(y_pred,1))
  f1_scores = []
  recall_scores = []
  precision_scores = []
  accuracy_scores = []
  target = []

  for label_tag , value in cat_target.items():
    true = []
    pred = []

    for true_val, pred_val in zip(y_true, y_pred):
      if true_val == value:
        true.append(1)
      else:
        true.append(0)
      if pred_val == value:
        pred.append(1)
      else:
        pred.append(0)

    accuracy = round(accuracy_score(true, pred),2)
    f1 = round(f1_score(true, pred),2)
    precision = round(precision_score(true,pred),2)
    recall = round(recall_score(true, pred),2)

    # append scores and appropriate target.
    target.append(label_tag)
    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    precision_scores.append(precision)
    recall_scores.append(recall)


  print('         |       Accuracy Score|        Precision|        Recall|       F1|')
  print('-----------------------------------------------------------------------------------')
  for i in range(len(cat_target.keys())):
    print(f'{target[i]}             {accuracy_scores[i]}                 {precision_scores[i]}                 {recall_scores[i]}                   {f1_scores[i]}  ')


y_pred = model.predict(X_test)

calculate_metrics(case_cat_targ, y_test, y_pred)
```

```
         |       Accuracy Score|        Precision|        Recall|       F1|
-----------------------------------------------------------------------------------
T             0.99                 0.96                 0.91                   0.94  
N             0.99                 0.99                 1.0                   0.99  

```

## Tying It All Together
We are going to combine the predictive power of all the models and their processing pipelines into one function. We define all the functions needed for this in the code snippet below:

```python

def convert_predictions(pred, label_dict):
  result = []
  pred  = np.argmax(pred,1)
  for val in list(pred):
    for label, value in label_dict.items():
      if val == value:
        result.append(label)

  return result

def process_word_results(chars,predictions):
  #targets are : S for single word, E for end of word, I for inside word, B for beginning of word.
  print('processing text into words ...')
  full_text = ''
  for char , prediction in zip(chars, predictions):
    if prediction == 'S':
      full_text += ' '
      full_text += char
      full_text += ' '
    elif prediction == 'E':
      full_text += char
      full_text += ' '
    elif prediction == 'B':
      full_text += ' '
      full_text += char
    else:
      full_text += char

  print('Word segmentation completed.')
  print('Now processing words into sentences...')
  return full_text


def process_sent_result(words, predictions):
  # For the sentences, we are going to tag a word with 'E' if it ends the sentence (comes before the full stop)
  # and tag a word with 'P' (pause) if it comes before a comma and tag a word inside a sentence with an 'I'
  print('Constructing sentences and adding commas now ....')
  full_text = ' '
  for word, prediction in zip(words, predictions):
    if prediction == 'E':
      full_text += word
      full_text += '. '
    elif prediction == 'P':
      full_text += word
      full_text += ', '
    else:
      full_text += word
      full_text += ' '

  print('Sentence successfully constructed.')
  print('Now preparing sentences and casing relevant words..')
  print('Please, have some popcorn while you wait...')
  return full_text

def process_case_result(words, predictions):
  # For true casing of words in sentecnes , we are going to tag a word as 'T' for titled if the first letter of the word is in capital
  # and tag it 'N' for not titled if the first letter isnt a capital letter.
  print('Casing sentences appropriately...')
  full_text = ' '
  for word , prediction in zip(words, predictions):
    if prediction == 'T':
      full_text += word.capitalize()
      full_text += ' '
    else:
      full_text += word
      full_text += ' '

  print('Sentences completey cased. Now returning result... ')
  return full_text

# function that changes any new tag in a new corpus to an UNK value.
def change_tag(tag_list, tag, least_tag):
  if tag not in tag_list:
    return least_tag
  else:
    return tag

from time import time
def process_raw_text(raw_text,fill_tag = 'WRB+DOZ', FILE_PATH=FILE_PATH):
  # The full text here is raw text without any processing (just letters with no space)
  #assert type(raw_text) == str
  start_time = time()
  print('Processing data for word segmentation...\n')
  full_text = list(raw_text)
  full_text = pd.DataFrame({'Characters': full_text})
  # extract context
  print('Extracting features for each character in text...\n')
  #full_text = extract_context(full_text,n_gram=2)
  full_text = process_char_dataset(full_text,n_gram=2)

  # Load transformer
  print('Transforming characters...\n')
  try:
    char_transformer = load(os.path.join(FILE_PATH,'char_pipeline_transformer'))
  except:
    print('Character transformer not found in the current directory.\n')
    return

  full_text = char_transformer.transform(full_text)

  # # predict with trained model.
  # # Load model first
  print('Loading model and predicting character identities... \n')
  try:
    model = tf.keras.models.load_model(os.path.join(FILE_PATH,'word_segmentor'))
  except:
    print('word segmentor model not found in the current directory.\n')
    return
  predictions = model.predict(full_text)

  # # process result here
  print('Processing results and segmenting characters into words.....\n\n')
  predictions = convert_predictions(predictions, word_cat_targ )
  #print('Char predictions' , predictions[:5])
  full_text = process_word_results(list(raw_text), predictions)
  #print('processed to word', full_text[:15])
  word_time = time()
  print(f'Done (completed in {(word_time - start_time)} seconds).\n')


  # # Process data for sentence model
  print('Now, processing words for sentence identification and segmentation... \n')
  full_text = nltk.word_tokenize(full_text)
  word_to_tag = nltk.pos_tag(full_text)
  tags = []
  words = []
  for word, tag in word_to_tag:
    words.append(word)
    tags.append(tag)

  # # Process data for sentence transformer model
  full_text = pd.DataFrame({'words': words, 'tags': tags})
  #rint(len(full_text['tags'].unique()))
  # Load tags list
  tag_list = load(os.path.join(FILE_PATH,'tags_list'))

  # change previously unseen tags in this new corpus to the least tag during training of our model.
  # The assumption is that this unseen tags would also occur less in this new data corpus.
  full_text['tags'] = full_text['tags'].apply(lambda x: change_tag(tag_list,x,fill_tag))

  print('Processing words and transforming words appropriately...\n')
  full_text = process_sent_data(full_text, n_gram=2)

  print('Loading transformer and transforming data...')
  try:
    sent_transformer = load(os.path.join(FILE_PATH, 'sentence_transformer'))
  except:
    print('sentence transformer not found in current directory.')
    return

  full_text = sent_transformer.transform(full_text)

  # Load sentence model
  print('Loading model and predicting word identities...\n')
  try:
    model = tf.keras.models.load_model(os.path.join(FILE_PATH,'Comma_sentence_segmentor'))
  except:
    print('sentence segmentor model not found in current directory.')
    return
  result = model.predict(full_text)

  # # process results here
  print('Processing results and segmenting words to sentences appropriately...\n')
  predictions = convert_predictions(result, sent_cat_targ )
  full_text = process_sent_result(words, predictions)
  sent_time = time()
  print(f'Done (completed in {(sent_time - word_time)/60} mins).\n ')

  # process and preprocess text for the casing transformer
  print('Finally, processing sentences for appropriate casing of words...\n')
  full_text = nltk.word_tokenize(full_text)
  text_to_tag = nltk.pos_tag(full_text)
  tags = []
  words = []

  for word , tag in text_to_tag:
    words.append(word)
    tags.append(tag)

  full_text = pd.DataFrame({'words': words , 'tags': tags})
  # Load tags list
  case_tag_list = load(os.path.join(FILE_PATH,'case_tag_list'))
  # change previously unseen tags to 'WRB+DOZ' which was the least occuring tag during training.
  full_text['tags'] = full_text['tags'].apply(lambda x: change_tag(case_tag_list,x,least_tag))

  # process data for transformer model
  print('Processing words and sentences for transformer model...\n')
  full_text = process_case_data(full_text)

  # Transform data
  print('Loading and transforming processed data...\n')
  try:
    case_transformer = load(os.path.join(FILE_PATH, 'case_pipeline'))
    #print('case transformer loaded..')
  except:
    print('case transformer was not found in the current directory.\n')
    return

  full_text = case_transformer.transform(full_text)

  # Load casing model
  print('Loading model and predicting appropriate words for casing... \n')
  try:
    model = tf.keras.models.load_model(os.path.join(FILE_PATH, 'Casing_model'))
  except:
    print('casing model could not be found in the current directory.\n')
    return

  result = model.predict(full_text)

  # Process final result here
  print('Processing result and producing final format...\n')
  predictions = convert_predictions(result, case_cat_targ)
  full_text = process_case_result(words, predictions)
  case_time = time()
  print(f'Done (completed in {(case_time - sent_time)} seconds).\n)')
  print('Returning final processed result...\n\n')
  print(f'Whole task was completed in {(case_time - start_time)/60} mins.')
  print('-'*200)
  print('\n\n')

  return full_text
```

Lets test it all at once. We have our models saved to file.
We will use the first genre of the brown corpus for this. We will extract the text and remove all space, comma, full stop characters and casing from the text. Then we will run it through the <code>process_raw_text</code> function to intuitively analyze the performance of the models combined.


```python
corpus = brown.words(categories=brown.categories()[0])

text = ''
for word in corpus:
  text += word.lower()
text = text.replace('.','')
text = text.replace(',','')
text
```

This is what the raw text looks like when all the above are removed.
```
'danmorgantoldhimselfhewouldforgetannturnerhewaswellridofherhecertainlydidn'twantawifewhowasfickleasannifhehadmarriedherhe'dhavebeenaskingfortroublebutallofthiswasrationalizationsometimeshewokeupinthemiddleofthenightthinkingofannandthencouldnotgetbacktosleephisplansanddreamshadrevolvedaroundhersomuchandforsolongthatnowhefeltasifhehadnothingtheeasiestthingwouldbetosellouttoalbuddandleavethecountrybuttherewasastubbornstreakinhimthatwouldn'tallowitthebestantidoteforthebitternessanddisappointmentthatpoisonedhimwashardworkhefoundthatifhewastiredenoughatnighthewenttosleepsimplybecausehewastooexhaustedtostayawakeeachdayhefoundhimselfthinkinglessoftenofann;;eachdaythehurtwasalittledulleralittlelesspoignanthehadplentyofworktodobecausethesummerwasunusuallydryandhotthespringproducedasmallerstreamthaninordinaryyearsthegrassinthemeadowscamefastnowthatthewarmweatherwasherehecouldnotaffordtoloseadropofthepreciouswatersohespentmostofhiswakinghoursalongtheditchesinhismeadowshehadnoideahowmuchtimebuddwou...'
```

Now, let's run this text chunk through our series of models to process and reformat the text.

```python
# We will be using 'WRB+DOZ' as our fill_na values throughout.
process_raw_text(text, least_tag)
```

```
dan morgan told himself he would forget ann turner he was well rid of her he certainly did n't want a wife who was fickle as ann if he had married her he 'd have been asking for trouble but all of this was rationalization sometime she woke up in the middle of the night thinking of ann and then could not get back to sleep his plans and dreams had revolved around her so much and for so long that now he felt as if he had nothing the easiest thing would be to sell out to al budd and leave the country but there was a s tubborn streak in him that would n't allow it the bestant i dote for the bitterness and dis appointment that pois oned him was hard work he found that if he was tired enough at nighthe went to sleep simply because he was too exhausted to stay awake each day he found himself thinkingless oft en of ann ; ; each day the hurt was a little duller a little les s poignan the had plenty of work to do be cause the summer was unusually dry and hot the spring produced a smaller stream...'
```

We can see that our model did a really good job with processing this text here! Just amazing!!

## Conclusion

The culmination of these efforts resulted in a highly accurate system for processing unstructured raw text. With an accuracy of approximately 97%, this system can effectively segment words and sentences, as well as capitalize words correctly.

This system has the potential to revolutionize the way we interact with raw text. It can be used to pre-process data for various NLP tasks such as text classification, sentiment analysis, and machine translation.

The journey of developing this machine learning system has surely been an enriching and enlightening experience. It will not only deepen your understanding of NLP but also spark your passion for unlocking the secrets hidden within text.

I hope you've enjoyed this article series and found it both informative and engaging. If you have any questions or feedback, please feel free to <a href=""> contact me</a>.


