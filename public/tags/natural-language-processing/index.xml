<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on Jeffrey Paul</title>
    <link>http://localhost:1313/tags/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on Jeffrey Paul</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rotary Positional Embeddings</title>
      <link>http://localhost:1313/post/rope/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/rope/</guid>
      <description>Some texts&#xA;Demystifying Rope Embeddings: A Comprehensive Guide Embeddings have become a cornerstone in the field of natural language processing (NLP), helping machines understand and process human language. Among the various types of embeddings, rope embeddings, positional embeddings, and trainable embeddings play crucial roles. In this article, we will explore rope embeddings in depth, understand their purpose, and compare them with positional and trainable embeddings.&#xA;Introduction to Embeddings Embeddings are a way to represent words, phrases, or even sentences as continuous vectors in a high-dimensional space.</description>
    </item>
  </channel>
</rss>
