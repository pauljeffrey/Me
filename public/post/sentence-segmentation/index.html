<!DOCTYPE html>
<html lang=""><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2) | Jeffrey Paul</title>
    <meta name="description" content="Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2)">
    <meta property="og:site_name" content="Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2)" />
    <meta property="og:title" content="Jeffrey Paul" />
    <meta property="og:description" content="Title: Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2) In the Part 1 of this article, we focused on training a Neural Network that can segment a raw sequence (stream) of text characters into words and we were able to achieve an accuracy of about 95% using some complex feature engineering. By the way, if you have not checked out the Part 1 of this lovely project&amp;rsquo;s article, you can find it here ." />
    <meta property="og:image" content="http://localhost:1313/images/jeff2.jpg" />
    <meta name="keywords"
          content="" />
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
        integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
          crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
        integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
        crossorigin="anonymous">
    </script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);">
    </script>
    
    <meta name="keywords" content="fast, hugo, theme, minimal, gruvbox">
    <link rel="icon" type="image/svg" href='http://localhost:1313/img/logo.png' />
    <meta name="author" content='Jeffrey Paul'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.124.1">
    
    <link rel="stylesheet" href="http://localhost:1313/sass/main.min.a669fc379ca0ba4d389af69be2682407e7bca16d368f2e7ad5b83c0cd80029b3.css" type="text/css" media="screen">

    

    

    
    </head>
<body>
      <div class="line" id="scrollIndicator"></div>
      <div class="main"><div class="title">
  <div class="name">
    <h2><a href="http://localhost:1313/"
	   style="text-decoration: none; color: inherit;">Jeffrey Paul</a></h2>
  </div>
  <div class="color-scheme">
    <input type="checkbox" class="checkbox" id="chk" />
    <label class="label" for="chk">
						<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="moon" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M283.211 512c78.962 0 151.079-35.925 198.857-94.792 7.068-8.708-.639-21.43-11.562-19.35-124.203 23.654-238.262-71.576-238.262-196.954 0-72.222 38.662-138.635 101.498-174.394 9.686-5.512 7.25-20.197-3.756-22.23A258.156 258.156 0 0 0 283.211 0c-141.309 0-256 114.511-256 256 0 141.309 114.511 256 256 256z"></path></svg>
						<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="sun" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 160c-52.9 0-96 43.1-96
										96s43.1 96 96 96 96-43.1 96-96-43.1-96-96-96zm246.4 80.5l-94.7-47.3 33.5-100.4c4.5-13.6-8.4-26.5-21.9-21.9l-100.4 33.5-47.4-94.8c-6.4-12.8-24.6-12.8-31 0l-47.3 94.7L92.7 70.8c-13.6-4.5-26.5 8.4-21.9 21.9l33.5 100.4-94.7 47.4c-12.8 6.4-12.8 24.6 0 31l94.7 47.3-33.5 100.5c-4.5 13.6 8.4 26.5 21.9 21.9l100.4-33.5 47.3 94.7c6.4 12.8 24.6 12.8 31 0l47.3-94.7 100.4 33.5c13.6 4.5 26.5-8.4 21.9-21.9l-33.5-100.4 94.7-47.3c13-6.5 13-24.7.2-31.1zm-155.9 106c-49.9 49.9-131.1 49.9-181 0-49.9-49.9-49.9-131.1 0-181 49.9-49.9 131.1-49.9 181 0 49.9 49.9 49.9 131.1 0 181z"></path></svg>
      <div class="ball"></div>
    </label>
  </div>
</div>
<script>
  const themeSetter = (theme) => {
      document.body.classList.toggle('dark')
      localStorage.setItem('theme', theme)
      blockSwitcher()
  }

  const blockSwitcher = () => [...document.getElementsByTagName("BLOCKQUOTE")]
	.forEach(b => b.classList.toggle('dark'))

  const styleSwapper = () => {
      document.body.classList.add('back-transition')
      if (localStorage.getItem('theme') === 'dark') themeSetter('light')
      else if (localStorage.getItem('theme') === 'light') themeSetter('dark')
  }

  if (localStorage.getItem('theme') === 'dark'){
      themeSetter('dark')
      document.addEventListener("DOMContentLoaded", blockSwitcher)
  }
 else localStorage.setItem('theme', 'light')

  document.getElementById('chk').addEventListener('change',styleSwapper);

  window.addEventListener("scroll", () => {
      let height = document.documentElement.scrollHeight
          - document.documentElement.clientHeight;
      if(height >= 500){
	  let winScroll = document.body.scrollTop
              || document.documentElement.scrollTop;
	  let scrolled = (winScroll / height) * 100;
	  document.getElementById("scrollIndicator").style.width = scrolled + "%";
      }
  });
</script>

<section class="intro">
  
  <div class="post-header">
    <a class="go-back" href="http://localhost:1313/"><svg aria-hidden="true" focusable="false" data-prefix="far" class="back-icon" data-icon="caret-square-left" height="25px" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M272 157.1v197.8c0 10.7-13 16.1-20.5 8.5l-98.3-98.9c-4.7-4.7-4.7-12.2 0-16.9l98.3-98.9c7.5-7.7 20.5-2.3 20.5 8.4zM448 80v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V80c0-26.5 21.5-48 48-48h352c26.5 0 48 21.5 48 48zm-48 346V86c0-3.3-2.7-6-6-6H54c-3.3 0-6 2.7-6 6v340c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"></path></svg> </a>
    <h2 class="post-title">Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2)</h2>
</div>

<p>By <a href="">Paul Jeffrey</a></p>

<p class="post-dets">Published on: February 6, 2024
  | Reading Time: 14 min | Last Modified: February 6, 2024
  <br>
</p>
<span class="tags">
  
  <h5><a class="tag" href='http://localhost:1313/tags/neural%20networks'>neural networks</a></h5>
  
  <h5><a class="tag" href='http://localhost:1313/tags/natural%20language%20processing'>natural language processing</a></h5>
  
  <h5><a class="tag" href='http://localhost:1313/tags/word%20segmentation'>word segmentation</a></h5>
  
</span>

<div class="content">
  <h1 id="title-unraveling-the-secrets-of-raw-text-a-journey-through-word-sentence-segmentation-and-capitalization-with-python-part-2">Title: Unraveling the Secrets of Raw Text: A Journey Through Word, Sentence Segmentation and Capitalization with Python (Part 2)</h1>
<p>In the Part 1 of this article, we focused on training a Neural Network that can segment a raw sequence (stream) of text characters into words and we were able to achieve an accuracy of about 95% using some complex feature engineering. By the way, if you have not checked out the Part 1 of this lovely project&rsquo;s article, you can find it  here . I advice that you follow the first part of this project first before reading this. In this article, we are going to focus on predicting/forming full sentences given a corpus of words (without sentence demarcations) predicted by our &ldquo;word segmentor&rdquo; model from a raw sequence of text characters. We will call this text segmentation.</p>
<p>Example:
<code>My name is mary I am 10 years old I live in Nigeria</code>
is segmented into
<code>My name is mary. I am 10 years old. I live in Nigeria.</code></p>
<h2 id="introduction">Introduction</h2>
<p>Once the words have been predicted by the &ldquo;word segmentor&rdquo; model, the next challenge is to segment these words into meaningful sentences. This is crucial for understanding the context and flow of the text.</p>
<p>For this task, we utilize a combination of rule-based and machine learning techniques. Rule-based methods involve defining a set of rules that identify sentence boundaries based on punctuation marks, conjunctions, and other linguistic cues. Machine learning methods, on the other hand, train a model on annotated data to predict sentence boundaries. We will be making use of the brown corpus (found in the NLTK library) as we did in the last article.</p>
<p>Let&rsquo;s go!</p>
<p>We will be preparing data mostly in the same format as in the previous article to extract features from data. Here, we are also going to be extracting features from the neighbouring words.</p>
<p>In addition to all the calculations we made in the previous article during feature extraction, we are going to make use of the &lsquo;Part of Speech&rsquo; tags of each word here. This POS tags come with every word in the brown corpus. For the modelling of commas and full stop insertions, this POS tags play a huge role. For example, we know that many principles of use of the comma punctuation mark solely involves the identification of the part of speech of the words. We will also be using the length of words, tag congruity between neigbouring words etc. Everything is documented below. Please read through the code to understand the concepts and ideas.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># This function extracts a list of list of tagged_sentences from the brown_corpus</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Given its argument, it gets the whole corpus or the number of genres provided.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ext_tagged_corpus</span>(num_of_genre<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#print(&#39;yes&#39;)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> num_of_genre: <span style="color:#75715e"># if genre is given, then get only the sentences in that genre</span>
</span></span><span style="display:flex;"><span>    tagged_corpus <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_of_genre):
</span></span><span style="display:flex;"><span>      tagged_corpus<span style="color:#f92672">.</span>extend(brown<span style="color:#f92672">.</span>tagged_sents(categories<span style="color:#f92672">=</span>brown<span style="color:#f92672">.</span>categories()[i]))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tagged_corpus
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>: <span style="color:#75715e"># Else, get all sentences.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> brown<span style="color:#f92672">.</span>tagged_sents()
</span></span></code></pre></div><p>For the sentences, we are going to tag a word with &lsquo;E&rsquo; if it ends the sentence (comes before the full stop), and tag a word with &lsquo;P&rsquo; (pause) if it comes before a comma and tag a word inside a sentence with an &lsquo;I&rsquo;.</p>
<p>PS: Some functions here handle processing and generation of data for both sentence segmentation task and capitalization task which we will cover in part 3 of this series.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sent_cat_targ <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;E&#39;</span> : <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#39;P&#39;</span>: <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;I&#39;</span>: <span style="color:#ae81ff">2</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#For true casing of words in sentences , we are going to tag a word as &#39;T&#39; for titled if the #first letter of the word is in capital</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># and tag it &#39;N&#39; for not titled if the first letter isnt a capital letter.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>case_cat_targ <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;T&#39;</span> : <span style="color:#ae81ff">0</span> , <span style="color:#e6db74">&#39;N&#39;</span>: <span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This function creates the dataset, extracting all the words , part of speech tag associated with them and the target_tags in different lists.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_sent_data</span>(tagged_sent,casing<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>  words <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>  tags <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>  target_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> casing: <span style="color:#75715e"># if it is not the casing task (e.g John met the President) but segmentation task</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> each_sent <span style="color:#f92672">in</span> tagged_sent: <span style="color:#75715e"># Go through each sentence in the list of sentences.</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> index, word <span style="color:#f92672">in</span> enumerate(each_sent): <span style="color:#75715e"># Enumerate and go through words in each sentence</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> word[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;.&#39;</span> <span style="color:#f92672">or</span>  word[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;,&#39;</span>: <span style="color:#75715e"># if the current word is &#39;.&#39; or &#39;,&#39; then, continue to the next word</span>
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> index <span style="color:#f92672">&lt;</span> len(each_sent) <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>: <span style="color:#75715e"># if its not the last word (&#39;.&#39;) in this sentence.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">if</span> each_sent[index<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;.&#39;</span> : <span style="color:#75715e"># if the current word is not the last and the word in front of the current word is a &#39;.&#39; , append the appropriate tag</span>
</span></span><span style="display:flex;"><span>            target_data<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;E&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">elif</span> index <span style="color:#f92672">!=</span> each_sent[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">and</span> each_sent[index<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;,&#39;</span>: <span style="color:#75715e"># Do the same for &#39;,&#39;</span>
</span></span><span style="display:flex;"><span>            target_data<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;P&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">else</span>: <span style="color:#75715e"># Do the same for every other word</span>
</span></span><span style="display:flex;"><span>            target_data<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;I&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>          words<span style="color:#f92672">.</span>append(word[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>lower()) <span style="color:#75715e"># Append word in lower case</span>
</span></span><span style="display:flex;"><span>          tags<span style="color:#f92672">.</span>append(word[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> each_sent <span style="color:#f92672">in</span> tagged_sent: <span style="color:#75715e"># Go through each sentence in the list of sentences.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> index, word <span style="color:#f92672">in</span> enumerate(each_sent): <span style="color:#75715e"># Enumerate and go through words in each sentence</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">if</span> word[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>istitle() : <span style="color:#75715e"># if the first letter of the word is cased</span>
</span></span><span style="display:flex;"><span>            target_data<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;T&#39;</span>) <span style="color:#75715e"># return &#39;T&#39; for titled</span>
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">else</span>: <span style="color:#75715e"># Else return &#39;N&#39; for not titled</span>
</span></span><span style="display:flex;"><span>            target_data<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;N&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>          words<span style="color:#f92672">.</span>append(word[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>lower())
</span></span><span style="display:flex;"><span>          tags<span style="color:#f92672">.</span>append(word[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> words, tags, target_data <span style="color:#75715e"># return all words, their tags and their target_data.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Function that calculates the length of a word.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">word_length</span>(word):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> len(word)
</span></span></code></pre></div><p>Now, let&rsquo;s check out the length of our training corpus</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>corpus <span style="color:#f92672">=</span> ext_tagged_corpus(<span style="color:#ae81ff">8</span>) <span style="color:#75715e"># extract training corpus</span>
</span></span><span style="display:flex;"><span>len(corpus)
</span></span></code></pre></div><pre tabindex="0"><code>35104
</code></pre><p>Next, we extract words, tags and target data from this corpus below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>words, tags , target_data <span style="color:#f92672">=</span> create_sent_data(corpus) <span style="color:#75715e"># extract the words, various tags and target data</span>
</span></span><span style="display:flex;"><span>print(len(words) , len(target_data))
</span></span></code></pre></div><pre tabindex="0"><code>657589 657589
</code></pre><p>Let&rsquo;s take a look at the words and their corresponding tags.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sent_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;words&#39;</span>: words, <span style="color:#e6db74">&#39;tags&#39;</span>: tags})
</span></span><span style="display:flex;"><span>sent_df<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><!-- raw HTML omitted -->
<p>Now, let&rsquo;s save the a list of the unique tags to a file permananently. We do this so that our models never return an error when they come across an entirely new data that has a tag it has never seen before. This way, we can set these tags to &lsquo;UNK&rsquo; values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tag_list <span style="color:#f92672">=</span> sent_df[<span style="color:#e6db74">&#39;tags&#39;</span>]<span style="color:#f92672">.</span>unique()
</span></span><span style="display:flex;"><span>save_to_file(tag_list, os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(FILE_PATH,<span style="color:#e6db74">&#39;tags_list&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Number of unique words in dataset: &#39;</span>, len(sent_df[<span style="color:#e6db74">&#39;words&#39;</span>]<span style="color:#f92672">.</span>unique()))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Number of unique tags in dataset: &#39;</span>, len(sent_df[<span style="color:#e6db74">&#39;tags&#39;</span>]<span style="color:#f92672">.</span>unique()))
</span></span></code></pre></div><pre tabindex="0"><code>Number of unique words in dataset:  38191
Number of unique tags in dataset:  437
</code></pre><p>We have about 38,191 unique words and it will be difficult to create a one_hot_encoding with this amount because the matrix will be very scarce. Instead, we could create a co-occurrence matrix (function defined below) to scale the data with StandardScaler() and then perform a PCA transform to reduce the dimensionality to a few hundred columns. This will help us represent the words uniquely. The co-occurrrence matrix will be a (38191 x 38191) matrix since we have 38,191 unique words in our extracted corpus. Then we can reduce this dimension using PCA to a few hundreds.</p>
<p>This will require setting a unique representation for any unknown (new) word the model will encounter during test. We can do this by replacing the least occuring word in the train set with an &lsquo;UNK&rsquo; tag for example.</p>
<p>However, to reduce computational complexity, we are going to represent each word by its word length, frequency, probability in corpus, tagset,tagset of neigbouring words, transitional probabilities into neigbouring words and probabilities of the various sequences. We do this because we also know that the frequency and probability of the word in the whole corpus is unique to each word no matter where they occur.</p>
<p>I believe that this may reduce the performance of the model. This is because this model makes some assumptions. one of the very obvious one is that every unique word has a unique freq and probability of occurrence.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># I do not advise this for this task though.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">co_occurrence</span>(words, window_size): <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>    d <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    vocab <span style="color:#f92672">=</span> set()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i,word <span style="color:#f92672">in</span> enumerate(words):
</span></span><span style="display:flex;"><span>      vocab<span style="color:#f92672">.</span>add(word)  <span style="color:#75715e"># add to vocab</span>
</span></span><span style="display:flex;"><span>      next_token <span style="color:#f92672">=</span> words[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span> : i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>window_size]
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> next_token:
</span></span><span style="display:flex;"><span>        key <span style="color:#f92672">=</span> tuple( sorted([t, word]) )
</span></span><span style="display:flex;"><span>        d[key] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># formulate the dictionary into dataframe</span>
</span></span><span style="display:flex;"><span>    vocab <span style="color:#f92672">=</span> sorted(vocab) <span style="color:#75715e"># sort vocab</span>
</span></span><span style="display:flex;"><span>    df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(data<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros((len(vocab), len(vocab)), dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>int16),
</span></span><span style="display:flex;"><span>                      index<span style="color:#f92672">=</span>vocab,
</span></span><span style="display:flex;"><span>                      columns<span style="color:#f92672">=</span>vocab)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> key, value <span style="color:#f92672">in</span> d<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>      df<span style="color:#f92672">.</span>at[key[<span style="color:#ae81ff">0</span>], key[<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">=</span> value
</span></span><span style="display:flex;"><span>      df<span style="color:#f92672">.</span>at[key[<span style="color:#ae81ff">1</span>], key[<span style="color:#ae81ff">0</span>]] <span style="color:#f92672">=</span> value
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> df
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sent_df[<span style="color:#e6db74">&#39;word_length&#39;</span>] <span style="color:#f92672">=</span> sent_df[<span style="color:#e6db74">&#39;words&#39;</span>]<span style="color:#f92672">.</span>apply(word_length) <span style="color:#75715e"># calculate length of words.</span>
</span></span><span style="display:flex;"><span>sent_df<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><p>Let&rsquo;s find the least occuring tag in the &rsquo;tags&rsquo; column. We wiil use this to represent any new tag the transformer model we meet in the future when it encounters an entirely new text corpus.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sent_df<span style="color:#f92672">.</span>groupby(<span style="color:#e6db74">&#39;tags&#39;</span>)<span style="color:#f92672">.</span>count()
</span></span></code></pre></div><!-- raw HTML omitted -->
<p>We see that the &lsquo;WRB+DOZ&rsquo; occurs just once in the corpus. We shall use this as our fill_value.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>least_tag <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;WRB+DOZ&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># we convert the word_length to categories of exactly 1, 2 or greater than 2</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">convert_length</span>(word_length):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> word_length <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">elif</span> word_length <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Function gets the universal (non-specific) POS tags for each word. e.g &#39;NOUN&#39; instead of &#39;NP&#39; that stands for &#39;proper noun&#39;.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_universal_tag</span>(word):
</span></span><span style="display:flex;"><span>  tag <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>pos_tag([word],tagset<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;universal&#39;</span>)
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#print(tag)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> tag[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># function checks if index word is a punctuation. We do this because we are sure that punctuations cant exist before a &#39;.&#39; or &#39;,&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">is_punc</span>(word):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> (word <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;?&#39;</span> ) <span style="color:#f92672">or</span> (word <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;!&#39;</span>) <span style="color:#f92672">or</span> (word <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;:&#39;</span>) <span style="color:#f92672">or</span> (word <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;;&#39;</span>) <span style="color:#f92672">or</span> (word <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;-&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># function check if preceeding tag = tag of index character  = subsequent tag</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This very important for comma insertion . If the have the same tag, probably they are a set of listed items</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># e.g I have mangoes ,apples, pineapples. This statement contains a list of 3 proper nouns.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_tag_congruity</span>(row):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> row[<span style="color:#e6db74">&#39;pre-1 tags&#39;</span>] <span style="color:#f92672">==</span> row[<span style="color:#e6db74">&#39;tags&#39;</span>] <span style="color:#f92672">and</span> row[<span style="color:#e6db74">&#39;tags&#39;</span>] <span style="color:#f92672">==</span> row[<span style="color:#e6db74">&#39;post-1 tags&#39;</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">elif</span> row[<span style="color:#e6db74">&#39;pre-1 tags&#39;</span>] <span style="color:#f92672">==</span> row[<span style="color:#e6db74">&#39;tags&#39;</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">elif</span> row[<span style="color:#e6db74">&#39;tags&#39;</span>] <span style="color:#f92672">==</span> row[<span style="color:#e6db74">&#39;post-1 tags&#39;</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This function process the whole data for the subsequent transformers in our pipeline.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># A lot of the functions used for the word segmentation model were also used here so they represent the same</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># basic calculations and extractions.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_sent_data</span>(df,n_gram<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,fill_value <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;WRB+DOZ&#39;</span>):
</span></span><span style="display:flex;"><span>  df[<span style="color:#e6db74">&#39;word_length&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;words&#39;</span>]<span style="color:#f92672">.</span>apply(word_length)
</span></span><span style="display:flex;"><span>  df[<span style="color:#e6db74">&#39;word_length_class&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;word_length&#39;</span>]<span style="color:#f92672">.</span>apply(convert_length)
</span></span><span style="display:flex;"><span>  df[<span style="color:#e6db74">&#39;uni_tags&#39;</span>]  <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;words&#39;</span>]<span style="color:#f92672">.</span>apply(get_universal_tag)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> cal_freq_prob(df, <span style="color:#e6db74">&#39;words&#39;</span>,index<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,return_freq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e"># frequency and prob of &#39;words&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> cal_freq_prob(df, <span style="color:#e6db74">&#39;tags&#39;</span>, return_freq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e"># freq and prob of &#39;tags&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># block of code extract context and its features (words around index word)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># We are not bothered about the fill value for the words column here because the &#39;words&#39; column will be deleted later.</span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> extract_context(df, <span style="color:#e6db74">&#39;words&#39;</span>,<span style="color:#e6db74">&#39;words&#39;</span>, n_gram<span style="color:#f92672">=</span>n_gram,fill_value<span style="color:#f92672">=</span>fill_value)
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> extract_context(df,<span style="color:#e6db74">&#39;tags&#39;</span>,<span style="color:#e6db74">&#39;tags&#39;</span>,n_gram<span style="color:#f92672">=</span>n_gram, fill_value<span style="color:#f92672">=</span>fill_value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># # block of code generates sequences by concatenating the index word to the &#39;before&#39; and &#39;after&#39; words</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># # block of code also calculates other properties : transitional prob, prob. of word sequence etc</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Create pre, post and whole sequence</span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> process_seq_columns(df,<span style="color:#e6db74">&#39;tags&#39;</span>,<span style="color:#e6db74">&#39;tags&#39;</span>,n_gram)
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> process_seq_columns(df, <span style="color:#e6db74">&#39;words&#39;</span>,<span style="color:#e6db74">&#39;words&#39;</span>,n_gram)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Calculate frequency, prob of sequence and transitional probability for each character.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># We check tag congruity between the pre-1 tag, index tag and post-1 tag</span>
</span></span><span style="display:flex;"><span>  df[<span style="color:#e6db74">&#39;tag_congruity&#39;</span>]  <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>apply(check_tag_congruity,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Check that the current word is a punctuation mark</span>
</span></span><span style="display:flex;"><span>  df[<span style="color:#e6db74">&#39;is_punc&#39;</span>] <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;words&#39;</span>]<span style="color:#f92672">.</span>apply(is_punc)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># calculate frequency , probability of occurence of all extracted sequences.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#39;tags&#39;</span>, <span style="color:#e6db74">&#39;words&#39;</span>]:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, n_gram <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>      df <span style="color:#f92672">=</span> cal_freq_prob(df, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;pre-</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> seq </span><span style="color:#e6db74">{</span>col<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>,return_freq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,seq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e">#for presequence</span>
</span></span><span style="display:flex;"><span>      df <span style="color:#f92672">=</span> cal_freq_prob(df, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;post-</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> seq </span><span style="color:#e6db74">{</span>col<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>,return_freq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,seq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e"># for post sequence</span>
</span></span><span style="display:flex;"><span>      df <span style="color:#f92672">=</span> cal_freq_prob(df,<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;whole-seq </span><span style="color:#e6db74">{</span>col<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>,return_freq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,seq<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e"># for whole sequence with index in middle</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># calculate the trans. prob. of the index character to the next and for increasing order of sequence to the next character.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Do the same for the &#39;tags&#39; column.</span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> cal_transitional_prob(df, <span style="color:#e6db74">&#39;words&#39;</span> ,n_gram)
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> cal_transitional_prob(df, <span style="color:#e6db74">&#39;tags&#39;</span>, n_gram)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Since we represent words here by their freq and prob, to give each word more uniqueness, we shift the columns up and down</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># By a number of steps so they get infomation about the characters that exist before and after them.</span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> extract_context(df,<span style="color:#e6db74">&#39;words-freq&#39;</span>,<span style="color:#e6db74">&#39;words-freq&#39;</span>,n_gram<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, fill_value<span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> extract_context(df,<span style="color:#e6db74">&#39;words-prob&#39;</span>,<span style="color:#e6db74">&#39;words-prob&#39;</span>,n_gram<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, fill_value<span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Now we drop all the columns we dont need after we have extracted and calculated the ones we need below:</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> n_gram <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>: <span style="color:#75715e"># if we considered only one character pre and post</span>
</span></span><span style="display:flex;"><span>    drop <span style="color:#f92672">=</span> [col <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> (col<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;-freq&#39;</span>) <span style="color:#f92672">and</span> (<span style="color:#e6db74">&#39;words&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> col))]
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;whole-seq words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;whole-seq tags&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;pre-1 seq words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;pre-1 seq tags&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;post-1 seq words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;post-1 seq tags&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;post-1 words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;pre-1 words&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>: <span style="color:#75715e"># if we considered more than one character pre and post.</span>
</span></span><span style="display:flex;"><span>    drop <span style="color:#f92672">=</span> [col <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> (col<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;-freq&#39;</span>) <span style="color:#f92672">and</span> (<span style="color:#e6db74">&#39;words&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> col))]  <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>        [ col <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> (col<span style="color:#f92672">.</span>startswith(<span style="color:#e6db74">&#39;pre-2&#39;</span>) <span style="color:#f92672">and</span> col<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;tags&#39;</span>))] <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>        [col <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> (col<span style="color:#f92672">.</span>startswith(<span style="color:#e6db74">&#39;post-2&#39;</span>) <span style="color:#f92672">and</span> col<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;tags&#39;</span>))] <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>        [col <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> (col<span style="color:#f92672">.</span>startswith(<span style="color:#e6db74">&#39;pre-2&#39;</span>) <span style="color:#f92672">and</span> col<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;words&#39;</span>))] <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>        [col <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns <span style="color:#66d9ef">if</span> (col<span style="color:#f92672">.</span>startswith(<span style="color:#e6db74">&#39;post-2&#39;</span>) <span style="color:#f92672">and</span> col<span style="color:#f92672">.</span>endswith(<span style="color:#e6db74">&#39;words&#39;</span>))]<span style="color:#75715e"># + \</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#[col for col in df.columns if (col.startswith(&#39;post-2&#39;) and col.endswith(&#39;tags&#39;))]</span>
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;whole-seq words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;whole-seq tags&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;pre-1 seq words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;pre-1 seq tags&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;post-1 seq words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;post-1 seq tags&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;post-1 words&#39;</span>)
</span></span><span style="display:flex;"><span>    drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;pre-1 words&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  drop<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;words&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df <span style="color:#f92672">=</span> drop_column(df, drop)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> df
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sent_cat <span style="color:#f92672">=</span> [ <span style="color:#e6db74">&#39;tags&#39;</span>, <span style="color:#e6db74">&#39;word_length&#39;</span>, <span style="color:#e6db74">&#39;uni_tags&#39;</span>, <span style="color:#e6db74">&#39;pre-1 tags&#39;</span>, <span style="color:#e6db74">&#39;post-1 tags&#39;</span>, <span style="color:#e6db74">&#39;tag_congruity&#39;</span>, <span style="color:#e6db74">&#39;is_punc&#39;</span>]
</span></span></code></pre></div><p>Now, we compute and extract all our features so they can be preprocessed into the format that is acceptable to our machine learning model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train <span style="color:#f92672">=</span> process_sent_data(sent_df, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># process the data.</span>
</span></span><span style="display:flex;"><span>train<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><!-- raw HTML omitted -->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(len(train[<span style="color:#e6db74">&#39;tags&#39;</span>]<span style="color:#f92672">.</span>unique()))
</span></span><span style="display:flex;"><span>print(len(train[<span style="color:#e6db74">&#39;pre-1 tags&#39;</span>]<span style="color:#f92672">.</span>unique()))
</span></span><span style="display:flex;"><span>print(len(train[<span style="color:#e6db74">&#39;post-1 tags&#39;</span>]<span style="color:#f92672">.</span>unique()))
</span></span></code></pre></div><pre tabindex="0"><code>437
437
437
</code></pre><p>Before preprocessing data, we need to include the &lsquo;UNK&rsquo; tag we used to fill up the &lsquo;pre&rsquo; and &lsquo;post&rsquo; tags column we created to the &rsquo;tags&rsquo; column. We do this so they all have equal representation dimensions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Extract the unique values in categories and append &#39;UNK&#39; to the tags column&#39;s unique values</span>
</span></span><span style="display:flex;"><span>sent_cat_values <span style="color:#f92672">=</span> [list(train[col]<span style="color:#f92672">.</span>unique()) <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> sent_cat ]
</span></span><span style="display:flex;"><span>sent_cat_values[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;UNK&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We sort all the values in each category because the Ordinal Encoder will not accept numerical values that are not sorted.</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> cat <span style="color:#f92672">in</span> sent_cat_values:
</span></span><span style="display:flex;"><span>  cat<span style="color:#f92672">.</span>sort()
</span></span></code></pre></div><p>We then preprocess data using the same function we used for the word segmentation model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train , pipeline <span style="color:#f92672">=</span> preprocess_data(train, sent_cat, pipe_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sentence_transformer&#39;</span>)
</span></span><span style="display:flex;"><span>train
</span></span></code></pre></div><pre tabindex="0"><code>&lt;657589x1393 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;
	with 19760410 stored elements in Compressed Sparse Row format&gt;
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Convert target_data variables from string format to numerical format</span>
</span></span><span style="display:flex;"><span>target_data <span style="color:#f92672">=</span> to_cat(target_data, sent_cat_targ)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># change target to categorical format (One hot encoding)</span>
</span></span><span style="display:flex;"><span>target_data <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>to_categorical(target_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split data into train and test set</span>
</span></span><span style="display:flex;"><span>X_train, X_test , y_train, y_test  <span style="color:#f92672">=</span> train_test_split(train, target_data, test_size<span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>                                                      random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>,shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p>Now, Let&rsquo;s create our deep learning model for training. I also suggest the use of SVMs for training because they should work well as classifiers when data is represented in a high dimensional format. Xgboost is another option to consider.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Get input and output dim</span>
</span></span><span style="display:flex;"><span>input_dim <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>output_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span> <span style="color:#75715e"># because we have only 3 targets</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> create_model(input_dim, <span style="color:#ae81ff">1600</span>, <span style="color:#ae81ff">1000</span>, output_dim)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>summary()
</span></span></code></pre></div><pre tabindex="0"><code>Model: &#34;sequential_1&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_4 (Dense)             (None, 1600)              2230400   
                                                                 
 dense_5 (Dense)             (None, 1000)              1601000   
                                                                 
 dense_6 (Dense)             (None, 1000)              1001000   
                                                                 
 dense_7 (Dense)             (None, 3)                 3003      
                                                                 
=================================================================
Total params: 4835403 (18.45 MB)
Trainable params: 4835403 (18.45 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre><p>Let&rsquo;s train the model now!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Schedule early stopping so that model can be stopped when there is no longer improvement</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># after at least 5 steps.</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>batch_size  <span style="color:#f92672">=</span> <span style="color:#ae81ff">512</span>
</span></span><span style="display:flex;"><span>es <span style="color:#f92672">=</span> EarlyStopping(monitor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;val_loss&#39;</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;min&#39;</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, patience<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Schedule learning rate when you hit a plateau</span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> ReduceLROnPlateau(monitor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;val_loss&#39;</span>, factor<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, patience<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, min_delta<span style="color:#f92672">=</span><span style="color:#ae81ff">1E-7</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(X_train, y_train, epochs<span style="color:#f92672">=</span>epochs, batch_size<span style="color:#f92672">=</span>batch_size,
</span></span><span style="display:flex;"><span>                    validation_data<span style="color:#f92672">=</span>(X_test, y_test),callbacks<span style="color:#f92672">=</span>[es,lr])
</span></span></code></pre></div><pre tabindex="0"><code>Epoch 1/20
1028/1028 [==============================] - 27s 23ms/step - loss: 0.1124 - accuracy: 0.9577 - val_loss: 0.0985 - val_accuracy: 0.9631 - lr: 0.0010
Epoch 2/20
1028/1028 [==============================] - 22s 21ms/step - loss: 0.1006 - accuracy: 0.9607 - val_loss: 0.0956 - val_accuracy: 0.9639 - lr: 0.0010
Epoch 3/20
1028/1028 [==============================] - 16s 14ms/step - loss: 0.0965 - accuracy: 0.9619 - val_loss: 0.0952 - val_accuracy: 0.9644 - lr: 0.0010
Epoch 4/20
1028/1028 [==============================] - 16s 14ms/step - loss: 0.0928 - accuracy: 0.9631 - val_loss: 0.0959 - val_accuracy: 0.9637 - lr: 0.0010
Epoch 5/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0895 - accuracy: 0.9638 - val_loss: 0.1010 - val_accuracy: 0.9634 - lr: 0.0010
Epoch 6/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0861 - accuracy: 0.9651 - val_loss: 0.1011 - val_accuracy: 0.9638 - lr: 0.0010
Epoch 7/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0773 - accuracy: 0.9678 - val_loss: 0.1139 - val_accuracy: 0.9626 - lr: 1.0000e-04
Epoch 8/20
1028/1028 [==============================] - 15s 14ms/step - loss: 0.0741 - accuracy: 0.9689 - val_loss: 0.1226 - val_accuracy: 0.9628 - lr: 1.0000e-04
Epoch 8: early stopping
</code></pre><p>Let&rsquo;s plot the traning history.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pyplot<span style="color:#f92672">.</span>plot(history<span style="color:#f92672">.</span>history[<span style="color:#e6db74">&#39;loss&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;train&#39;</span>)
</span></span><span style="display:flex;"><span>pyplot<span style="color:#f92672">.</span>plot(history<span style="color:#f92672">.</span>history[<span style="color:#e6db74">&#39;val_loss&#39;</span>], label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;test&#39;</span>)
</span></span><span style="display:flex;"><span>pyplot<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>pyplot<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><code>sentence segmentor image here</code></p>
<p>Let&rsquo;s save our model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Comma_sentence_segmentor&#39;</span>
</span></span><span style="display:flex;"><span>model_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(FILE_PATH,model_name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>save(model_path)
</span></span></code></pre></div><p>We see that our model generalizes well and has about 96.6% accuracy on the validation set!</p>
<p>That&rsquo;s all for this part. Check out the last part of this series <!-- raw HTML omitted -->here<!-- raw HTML omitted -->. It is going to focus on True casing (Capitalization) of important words in the sentences formed. In the next article you will also get to see all of our trained models in action as they attempt to process a raw sequence stream of text characters to cased sentences which is very easy to read and understand.</p>
<p>See you <!-- raw HTML omitted -->there<!-- raw HTML omitted -->!.</p>

</div>

</section>
<footer id="footer">
    <strong></strong>
    <div class="social">
        &nbsp;
<a href="https://github.com/pauljeffrey" target="_blank" rel="me" title="Github" referrerpolicy="no-referrer">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
	stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
	<path
		d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
	</path>
</svg>
</a>
&nbsp;&nbsp;
<a href="drjeffreypaul@gmail.com" target="_blank" rel="me" title="Email" referrerpolicy="no-referrer">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
	stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
	<path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
	<polyline points="22,6 12,13 2,6"></polyline>
</svg>
</a>
&nbsp;&nbsp;
<a href="https://twitter.com/Jeffreypaul_" target="_blank" rel="me" title="Twitter" referrerpolicy="no-referrer">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
	stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
	<path
		d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z">
	</path>
</svg>
</a>
&nbsp;&nbsp;
<a href="https://www.linkedin.com/in/jeffreyotoibhi/" target="_blank" rel="me" title="Linkedin" referrerpolicy="no-referrer">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
	stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
	<path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
	<rect x="2" y="9" width="4" height="12"></rect>
	<circle cx="4" cy="4" r="2"></circle>
</svg>
</a>
&nbsp;
    </div><strong></strong>
    <p style="color:grey;"> 2024 Jeffrey Paul.  <a href="https://creativecommons.org/licenses/by/4.0/">Some rights reserved</a>.</p>
</footer>
</div>
    </body>
</html>
