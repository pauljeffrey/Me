<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jeffrey Paul</title>
    <link>http://localhost:1313/post/</link>
    <description>Recent content in Posts on Jeffrey Paul</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Mar 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Markdown Syntax Guide</title>
      <link>http://localhost:1313/post/markdown-syntax/</link>
      <pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/markdown-syntax/</guid>
      <description>&lt;p&gt;This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Placeholder Text</title>
      <link>http://localhost:1313/post/placeholder-text/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/placeholder-text/</guid>
      <description>&lt;p&gt;Lorem est tota propiore conpellat pectoribus de pectora summo.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Math Typesetting</title>
      <link>http://localhost:1313/post/math-typesetting/</link>
      <pubDate>Fri, 08 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/math-typesetting/</guid>
      <description>&lt;p&gt;Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Emoji Support</title>
      <link>http://localhost:1313/post/emoji-support/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/emoji-support/</guid>
      <description>&lt;p&gt;Emoji can be enabled in a Hugo project in a number of ways.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rotary Positional Embeddings</title>
      <link>http://localhost:1313/post/rope/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/rope/</guid>
      <description>Demystifying Rope Embeddings: A Comprehensive Guide Embeddings have become a cornerstone in the field of natural language processing (NLP), helping machines understand and process human language. Among the various types of embeddings, rope embeddings, positional embeddings, and trainable embeddings play crucial roles. In this article, we will explore rope embeddings in depth, understand their purpose, and compare them with positional and trainable embeddings.&#xA;Introduction to Embeddings Embeddings are a way to represent words, phrases, or even sentences as continuous vectors in a high-dimensional space.</description>
    </item>
  </channel>
</rss>
